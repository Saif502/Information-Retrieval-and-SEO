{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium\n",
        "!pip install webdriver-manager\n",
        "# Install Chrome and ChromeDriver\n",
        "!apt-get update\n",
        "!apt-get install -y chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dpQBImNz4_k",
        "outputId": "fbb3fd67-aa4d-40ab-ff2d-6bf8ad6c0fbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.31.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.4.0)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.1.31)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.13.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.16.0)\n",
            "Downloading selenium-4.31.0-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wsproto, outcome, trio, trio-websocket, selenium\n",
            "Successfully installed outcome-1.3.0.post0 selenium-4.31.0 trio-0.30.0 trio-websocket-0.12.2 wsproto-1.2.0\n",
            "Collecting webdriver-manager\n",
            "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from webdriver-manager) (2.32.3)\n",
            "Collecting python-dotenv (from webdriver-manager)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from webdriver-manager) (24.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->webdriver-manager) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->webdriver-manager) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->webdriver-manager) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->webdriver-manager) (2025.1.31)\n",
            "Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv, webdriver-manager\n",
            "Successfully installed python-dotenv-1.1.0 webdriver-manager-4.0.2\n",
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,543 kB]\n",
            "Get:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,155 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,872 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,244 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,704 kB]\n",
            "Get:16 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [46.3 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,844 kB]\n",
            "Fetched 20.8 MB in 3s (8,196 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  apparmor chromium-browser libfuse3-3 libudev1 snapd squashfs-tools\n",
            "  systemd-hwe-hwdb udev\n",
            "Suggested packages:\n",
            "  apparmor-profiles-extra apparmor-utils fuse3 zenity | kdialog\n",
            "The following NEW packages will be installed:\n",
            "  apparmor chromium-browser chromium-chromedriver libfuse3-3 snapd\n",
            "  squashfs-tools systemd-hwe-hwdb udev\n",
            "The following packages will be upgraded:\n",
            "  libudev1\n",
            "1 upgraded, 8 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 30.3 MB of archives.\n",
            "After this operation, 123 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 apparmor amd64 3.0.4-2ubuntu2.4 [598 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 squashfs-tools amd64 1:4.5-3build1 [159 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev1 amd64 249.11-0ubuntu3.15 [76.6 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.15 [1,557 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfuse3-3 amd64 3.10.5-1build1 [81.2 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 snapd amd64 2.67.1+22.04 [27.8 MB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-browser amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [49.2 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-chromedriver amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [2,308 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.5 [3,228 B]\n",
            "Fetched 30.3 MB in 2s (17.8 MB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package apparmor.\n",
            "(Reading database ... 126101 files and directories currently installed.)\n",
            "Preparing to unpack .../apparmor_3.0.4-2ubuntu2.4_amd64.deb ...\n",
            "Unpacking apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Selecting previously unselected package squashfs-tools.\n",
            "Preparing to unpack .../squashfs-tools_1%3a4.5-3build1_amd64.deb ...\n",
            "Unpacking squashfs-tools (1:4.5-3build1) ...\n",
            "Preparing to unpack .../libudev1_249.11-0ubuntu3.15_amd64.deb ...\n",
            "Unpacking libudev1:amd64 (249.11-0ubuntu3.15) over (249.11-0ubuntu3.12) ...\n",
            "Setting up libudev1:amd64 (249.11-0ubuntu3.15) ...\n",
            "Selecting previously unselected package udev.\n",
            "(Reading database ... 126301 files and directories currently installed.)\n",
            "Preparing to unpack .../udev_249.11-0ubuntu3.15_amd64.deb ...\n",
            "Unpacking udev (249.11-0ubuntu3.15) ...\n",
            "Selecting previously unselected package libfuse3-3:amd64.\n",
            "Preparing to unpack .../libfuse3-3_3.10.5-1build1_amd64.deb ...\n",
            "Unpacking libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Selecting previously unselected package snapd.\n",
            "Preparing to unpack .../snapd_2.67.1+22.04_amd64.deb ...\n",
            "Unpacking snapd (2.67.1+22.04) ...\n",
            "Setting up apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Created symlink /etc/systemd/system/sysinit.target.wants/apparmor.service → /lib/systemd/system/apparmor.service.\n",
            "Setting up squashfs-tools (1:4.5-3build1) ...\n",
            "Setting up udev (249.11-0ubuntu3.15) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Setting up snapd (2.67.1+22.04) ...\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.apparmor.service → /lib/systemd/system/snapd.apparmor.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.autoimport.service → /lib/systemd/system/snapd.autoimport.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.core-fixup.service → /lib/systemd/system/snapd.core-fixup.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.recovery-chooser-trigger.service → /lib/systemd/system/snapd.recovery-chooser-trigger.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Created symlink /etc/systemd/system/cloud-final.service.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Unit /lib/systemd/system/snapd.seeded.service is added as a dependency to a non-existent unit cloud-final.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.service → /lib/systemd/system/snapd.service.\n",
            "Created symlink /etc/systemd/system/timers.target.wants/snapd.snap-repair.timer → /lib/systemd/system/snapd.snap-repair.timer.\n",
            "Created symlink /etc/systemd/system/sockets.target.wants/snapd.socket → /lib/systemd/system/snapd.socket.\n",
            "Created symlink /etc/systemd/system/final.target.wants/snapd.system-shutdown.service → /lib/systemd/system/snapd.system-shutdown.service.\n",
            "Selecting previously unselected package chromium-browser.\n",
            "(Reading database ... 126530 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-browser_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "=> Installing the chromium snap\n",
            "==> Checking connectivity with the snap store\n",
            "===> System doesn't have a working snapd, skipping\n",
            "Unpacking chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package systemd-hwe-hwdb.\n",
            "Preparing to unpack .../systemd-hwe-hwdb_249.11.5_all.deb ...\n",
            "Unpacking systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Processing triggers for udev (249.11-0ubuntu3.15) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for dbus (1.12.20-2ubuntu4.1) ...\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GmBHOPrzbxN",
        "outputId": "e973ed02-86ba-41c8-bf21-b8d465545652"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: selenium in /usr/local/lib/python3.11/dist-packages (4.31.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.4.0)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.30.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.12.2)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.1.31)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.13.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.11/dist-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.16.0)\n",
            "Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "chromium-chromedriver is already the newest version (1:85.0.4183.83-0ubuntu2.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n",
            "Requirement already satisfied: webdriver-manager in /usr/local/lib/python3.11/dist-packages (4.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from webdriver-manager) (2.32.3)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from webdriver-manager) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from webdriver-manager) (24.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->webdriver-manager) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->webdriver-manager) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->webdriver-manager) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->webdriver-manager) (2025.1.31)\n",
            "\n",
            "Fetching article URLs from fact-checking sites...\n",
            "Fetching URLs from Snopes page 1...\n",
            "Failed to fetch Snopes page 1, status code: 404\n",
            "Fetching URLs from Snopes page 2...\n",
            "Failed to fetch Snopes page 2, status code: 404\n",
            "Fetching URLs from PolitiFact page 1...\n",
            "Fetching URLs from PolitiFact page 2...\n",
            "Fetching URLs from FactCheck.org page 1...\n",
            "Fetching URLs from FactCheck.org page 2...\n",
            "\n",
            "Found 0 Snopes articles\n",
            "Found 0 PolitiFact articles\n",
            "Found 20 FactCheck.org articles\n",
            "\n",
            "Crawling Snopes articles...\n",
            "\n",
            "Crawling PolitiFact articles...\n",
            "\n",
            "Crawling FactCheck.org articles...\n",
            "Crawling: https://www.factcheck.org/2025/04/legal-scholars-dispute-constitutional-loophole-for-a-third-trump-term/\n",
            "Successfully crawled: Legal Scholars Dispute Constitutional ‘Loophole’ for a Third Trump Term\n",
            "Crawling: https://www.factcheck.org/2025/03/viral-posts-share-phony-leaked-audio-of-vance-criticizing-musk/\n",
            "Successfully crawled: Viral Posts Share Phony ‘Leaked’ Audio of Vance Criticizing Musk\n",
            "Crawling: https://www.factcheck.org/2025/03/posts-fabricate-claims-about-trumps-response-to-dutertes-arrest/\n",
            "Successfully crawled: Posts Fabricate Claims About Trump’s Response to Duterte’s Arrest\n",
            "Crawling: https://www.factcheck.org/2025/03/study-focused-on-feminine-hygiene-products-not-transgender-men/\n",
            "Successfully crawled: Study Focused on Feminine Hygiene Products, Not Transgender Men\n",
            "Crawling: https://www.factcheck.org/2025/03/white-nationalist-group-is-still-active-contrary-to-social-media-claims/\n",
            "Successfully crawled: White Nationalist Group Is Still Active, Contrary to Social Media Claims\n",
            "Crawling: https://www.factcheck.org/2025/02/posts-spread-false-claim-about-doge-halting-supposed-obamacare-royalties/\n",
            "Successfully crawled: Posts Spread False Claim About DOGE Halting Supposed Obamacare ‘Royalties’\n",
            "Crawling: https://www.factcheck.org/2025/02/posts-share-bogus-audio-of-donald-trump-jr-supporting-arms-for-russia-not-ukraine/\n",
            "Successfully crawled: Posts Share Bogus Audio of Donald Trump Jr. Supporting Arms for Russia, Not Ukraine\n",
            "Crawling: https://www.factcheck.org/2025/02/online-posts-misconstrue-data-on-social-security-numbers/\n",
            "Successfully crawled: Online Posts Misconstrue Data on Social Security Numbers\n",
            "Crawling: https://www.factcheck.org/2025/02/trump-online-posts-misrepresent-government-subscriptions-to-news-services/\n",
            "Successfully crawled: Trump, Online Posts Misrepresent Government Subscriptions to News Services\n",
            "Crawling: https://www.factcheck.org/2025/02/social-media-posts-misidentify-pilot-killed-in-midair-collision-over-d-c/\n",
            "Successfully crawled: Social Media Posts Misidentify Pilot Killed in Midair Collision Over D.C.\n",
            "Crawling: https://www.factcheck.org/2025/01/trump-administration-makes-unsupported-claim-about-50-million-for-condoms-to-gaza/\n",
            "Successfully crawled: Trump Administration Makes Unsupported Claim About $50 Million for Condoms to Gaza\n",
            "Crawling: https://www.factcheck.org/2025/01/trump-order-didnt-reverse-all-of-bidens-measures-to-lower-drug-costs/\n",
            "Successfully crawled: Trump Order Didn’t Reverse All of Biden’s Measures to Lower Drug Costs\n",
            "Crawling: https://www.factcheck.org/2025/01/no-evidence-officer-who-shot-ashli-babbitt-was-pardoned-by-biden/\n",
            "Successfully crawled: No Evidence Officer Who Shot Ashli Babbitt Was Pardoned by Biden\n",
            "Crawling: https://www.factcheck.org/2025/01/canada-and-mexico-are-helping-to-fight-california-fires-contrary-to-meme/\n",
            "Successfully crawled: Canada and Mexico Are Helping to Fight California Fires, Contrary to Meme\n",
            "Crawling: https://www.factcheck.org/2025/01/770-payments-are-just-one-form-of-federal-aid-to-l-a-fire-victims/\n",
            "Successfully crawled: $770 Payments Are Just One Form of Federal Aid to L.A. Fire Victims\n",
            "Crawling: https://www.factcheck.org/2025/01/oregon-fire-trucks-fighting-l-a-blazes-didnt-require-emissions-testing/\n",
            "Successfully crawled: Oregon Fire Trucks Fighting L.A. Blazes Didn’t Require ‘Emissions Testing’\n",
            "Crawling: https://www.factcheck.org/2025/01/republicans-wrongly-tie-new-orleans-attack-to-illegal-immigration-suspect-was-a-citizen/\n",
            "Successfully crawled: Republicans Wrongly Tie New Orleans Attack to Illegal Immigration; Suspect Was a Citizen\n",
            "Crawling: https://www.factcheck.org/2024/12/gun-charges-against-hunter-biden-didnt-come-from-1994-crime-bill/\n",
            "Successfully crawled: Gun Charges Against Hunter Biden Didn’t Come from 1994 Crime Bill\n",
            "Crawling: https://www.factcheck.org/2024/12/posts-wrongly-conflate-u-s-china-prisoner-swap-with-bidens-recent-pardons/\n",
            "Successfully crawled: Posts Wrongly Conflate U.S.-China Prisoner Swap with Biden’s Recent Pardons\n",
            "Crawling: https://www.factcheck.org/2024/12/fox-news-commentator-had-aggressive-cancer-before-covid-19-vaccines-were-available/\n",
            "Successfully crawled: Fox News Commentator Had Aggressive Cancer Before COVID-19 Vaccines Were Available\n",
            "\n",
            "--- Crawling Complete ---\n",
            "Total articles collected: 20\n",
            "Articles by source:\n",
            "source_domain\n",
            "factcheck.org    20\n",
            "Name: count, dtype: int64\n",
            "Articles by verdict:\n",
            "verdict\n",
            "False      14\n",
            "Unrated     4\n",
            "True        2\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Dataset saved to 'fake_news_dataset.csv'\n",
            "Dataset saved to SQLite database 'fake_news.db'\n",
            "\n",
            "--- Sample Data ---\n",
            "\n",
            "--- Basic Data Analysis ---\n",
            "Verdict distribution:\n",
            "verdict\n",
            "False      14\n",
            "Unrated     4\n",
            "True        2\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Analyzing text content...\n",
            "Most common words in false claims:\n",
            "  social: 102\n",
            "  said: 90\n",
            "  was: 90\n",
            "  trump: 87\n",
            "  have: 79\n",
            "  media: 74\n",
            "  not: 71\n",
            "  jan: 70\n",
            "  feb: 69\n",
            "  from: 66\n",
            "  has: 65\n",
            "  about: 62\n",
            "  government: 61\n",
            "  are: 61\n",
            "  president: 58\n",
            "  our: 53\n",
            "  california: 53\n",
            "  biden: 53\n",
            "  news: 51\n",
            "  people: 51\n"
          ]
        }
      ],
      "source": [
        "# Fake News Crawler for Fact-Checking Websites (Google Colab Version)\n",
        "# This version uses a special Colab-compatible approach for Selenium\n",
        "\n",
        "# Install required packages\n",
        "!pip install selenium pandas requests beautifulsoup4\n",
        "\n",
        "# Additional installations specific for Colab environment\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin/\n",
        "!pip install webdriver-manager\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "import re\n",
        "from datetime import datetime\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
        "\n",
        "# Configure Chrome options specifically for Colab\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "chrome_options.add_argument('--disable-gpu')\n",
        "chrome_options.add_argument('--disable-extensions')\n",
        "chrome_options.add_argument('--disable-infobars')\n",
        "chrome_options.add_argument('--mute-audio')\n",
        "chrome_options.add_argument('--remote-debugging-port=9222')\n",
        "chrome_options.add_argument('--user-agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36\"')\n",
        "\n",
        "# Colab-specific WebDriver setup\n",
        "def setup_driver():\n",
        "    try:\n",
        "        # First method with System Path\n",
        "        driver = webdriver.Chrome(options=chrome_options)\n",
        "        return driver\n",
        "    except Exception as e1:\n",
        "        print(f\"First method failed: {e1}\")\n",
        "        try:\n",
        "            # Second method with service object\n",
        "            service = Service('/usr/bin/chromedriver')\n",
        "            driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "            return driver\n",
        "        except Exception as e2:\n",
        "            print(f\"Second method failed: {e2}\")\n",
        "            # Fallback to requests and BeautifulSoup if Selenium fails\n",
        "            print(\"Selenium setup failed, will use requests and BeautifulSoup instead\")\n",
        "            return None\n",
        "\n",
        "# Alternative approach using requests and BeautifulSoup if Selenium fails\n",
        "def crawl_with_requests(url, source_domain):\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36'\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Generic extraction logic that works across sites\n",
        "            title = \"\"\n",
        "            if soup.find('h1'):\n",
        "                title = soup.find('h1').text.strip()\n",
        "\n",
        "            # Extract paragraphs for content\n",
        "            paragraphs = soup.find_all('p')\n",
        "            article_text = ' '.join([p.text.strip() for p in paragraphs])\n",
        "\n",
        "            # Extract date - generic approach\n",
        "            date_published = \"Unknown\"\n",
        "            time_tag = soup.find('time')\n",
        "            if time_tag:\n",
        "                date_attr = time_tag.get('datetime')\n",
        "                if date_attr:\n",
        "                    date_published = date_attr.split('T')[0]\n",
        "                else:\n",
        "                    date_published = time_tag.text.strip()\n",
        "\n",
        "            # Try to find a verdict based on common patterns\n",
        "            verdict = \"Unknown\"\n",
        "            # Look for verdict-related elements\n",
        "            verdict_terms = ['false', 'true', 'pants on fire', 'mostly true', 'half true',\n",
        "                            'mostly false', 'pants-on-fire', 'mixture', 'unproven']\n",
        "\n",
        "            # Check for verdict in specific elements\n",
        "            rating_elements = soup.find_all(['span', 'div'], class_=lambda x: x and ('rating' in x.lower() or 'verdict' in x.lower()))\n",
        "            for element in rating_elements:\n",
        "                verdict = element.text.strip()\n",
        "                break\n",
        "\n",
        "            # If not found, check for verdict keywords in the article\n",
        "            if verdict == \"Unknown\":\n",
        "                lower_text = article_text.lower()\n",
        "                for term in verdict_terms:\n",
        "                    if term in lower_text:\n",
        "                        paragraph_with_term = next((p.text.strip() for p in paragraphs if term in p.text.lower()), None)\n",
        "                        if paragraph_with_term:\n",
        "                            # Take the sentence containing the verdict term\n",
        "                            sentences = paragraph_with_term.split('.')\n",
        "                            for sentence in sentences:\n",
        "                                if term in sentence.lower():\n",
        "                                    verdict = sentence.strip()\n",
        "                                    break\n",
        "                            break\n",
        "\n",
        "            # Extract claim\n",
        "            claim = \"See article for details\"\n",
        "            claim_elements = soup.find_all(['div', 'p'], class_=lambda x: x and ('claim' in x.lower()))\n",
        "            if claim_elements:\n",
        "                claim = claim_elements[0].text.strip()\n",
        "\n",
        "            return {\n",
        "                'title': title,\n",
        "                'claim': claim,\n",
        "                'verdict': verdict,\n",
        "                'article_text': article_text,\n",
        "                'url': url,\n",
        "                'date_published': date_published,\n",
        "                'date_crawled': datetime.now().strftime(\"%Y-%m-%d\"),\n",
        "                'source_domain': source_domain\n",
        "            }\n",
        "        else:\n",
        "            print(f\"Failed to retrieve {url}, status code: {response.status_code}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error crawling {url} with requests: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Create empty DataFrame to store results\n",
        "columns = ['title', 'claim', 'verdict', 'article_text', 'url', 'date_published', 'date_crawled', 'source_domain']\n",
        "fake_news_db = pd.DataFrame(columns=columns)\n",
        "\n",
        "# Function to get article URLs from Snopes\n",
        "def get_snopes_article_urls(num_pages=3):\n",
        "    article_urls = []\n",
        "    base_url = \"https://www.snopes.com/fact-check/page/{}/\"\n",
        "\n",
        "    for page in range(1, num_pages + 1):\n",
        "        try:\n",
        "            url = base_url.format(page)\n",
        "            print(f\"Fetching URLs from Snopes page {page}...\")\n",
        "\n",
        "            response = requests.get(url)\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "                # Find article links\n",
        "                article_cards = soup.find_all('article', class_='list-group-item')\n",
        "                for card in article_cards:\n",
        "                    link = card.find('a', href=True)\n",
        "                    if link and 'href' in link.attrs:\n",
        "                        article_urls.append(link['href'])\n",
        "\n",
        "                # Be polite with delay\n",
        "                time.sleep(random.uniform(1.0, 2.0))\n",
        "            else:\n",
        "                print(f\"Failed to fetch Snopes page {page}, status code: {response.status_code}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching Snopes page {page}: {str(e)}\")\n",
        "\n",
        "    return article_urls\n",
        "\n",
        "# Function to get article URLs from PolitiFact\n",
        "def get_politifact_article_urls(num_pages=3):\n",
        "    article_urls = []\n",
        "    base_url = \"https://www.politifact.com/factchecks/?page={}\"\n",
        "\n",
        "    for page in range(1, num_pages + 1):\n",
        "        try:\n",
        "            url = base_url.format(page)\n",
        "            print(f\"Fetching URLs from PolitiFact page {page}...\")\n",
        "\n",
        "            response = requests.get(url)\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "                # Find article links\n",
        "                article_items = soup.find_all('li', class_='o-listicle__item')\n",
        "                for item in article_items:\n",
        "                    link = item.find('a', class_='m-statement__link', href=True)\n",
        "                    if link and 'href' in link.attrs:\n",
        "                        article_urls.append('https://www.politifact.com' + link['href'] if not link['href'].startswith('http') else link['href'])\n",
        "\n",
        "                # Be polite with delay\n",
        "                time.sleep(random.uniform(1.0, 2.0))\n",
        "            else:\n",
        "                print(f\"Failed to fetch PolitiFact page {page}, status code: {response.status_code}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching PolitiFact page {page}: {str(e)}\")\n",
        "\n",
        "    return article_urls\n",
        "\n",
        "# Function to get article URLs from FactCheck.org\n",
        "def get_factcheck_article_urls(num_pages=3):\n",
        "    article_urls = []\n",
        "    base_url = \"https://www.factcheck.org/fake-news/page/{}/\"\n",
        "\n",
        "    for page in range(1, num_pages + 1):\n",
        "        try:\n",
        "            url = base_url.format(page)\n",
        "            print(f\"Fetching URLs from FactCheck.org page {page}...\")\n",
        "\n",
        "            response = requests.get(url)\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "                # Find article links\n",
        "                article_items = soup.find_all('article', class_='post')\n",
        "                for item in article_items:\n",
        "                    link = item.find('h3', class_='entry-title').find('a', href=True)\n",
        "                    if link and 'href' in link.attrs:\n",
        "                        article_urls.append(link['href'])\n",
        "\n",
        "                # Be polite with delay\n",
        "                time.sleep(random.uniform(1.0, 2.0))\n",
        "            else:\n",
        "                print(f\"Failed to fetch FactCheck.org page {page}, status code: {response.status_code}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching FactCheck.org page {page}: {str(e)}\")\n",
        "\n",
        "    return article_urls\n",
        "\n",
        "# Main crawling function that tries Selenium first, falls back to requests\n",
        "def crawl_article(url, source_domain, driver=None):\n",
        "    if driver is not None:\n",
        "        try:\n",
        "            # Try with Selenium\n",
        "            driver.get(url)\n",
        "            WebDriverWait(driver, 10).until(\n",
        "                EC.presence_of_element_located((By.TAG_NAME, \"h1\"))\n",
        "            )\n",
        "\n",
        "            # Extract data based on the source domain\n",
        "            if source_domain == 'snopes.com':\n",
        "                # Extract title\n",
        "                title = driver.find_element(By.CSS_SELECTOR, \"h1.title\").text.strip()\n",
        "\n",
        "                # Get rating/verdict\n",
        "                try:\n",
        "                    verdict = driver.find_element(By.CSS_SELECTOR, \"span.rating-label-with-symbol\").text.strip()\n",
        "                except NoSuchElementException:\n",
        "                    try:\n",
        "                        verdict = driver.find_element(By.CSS_SELECTOR, \"div.rating-wrapper\").text.strip()\n",
        "                    except:\n",
        "                        verdict = \"Unknown\"\n",
        "\n",
        "                # Get claim\n",
        "                try:\n",
        "                    claim = driver.find_element(By.CSS_SELECTOR, \"div.claim-text\").text.strip()\n",
        "                except:\n",
        "                    claim = \"No claim specified\"\n",
        "\n",
        "                # Get article text\n",
        "                try:\n",
        "                    paragraphs = driver.find_elements(By.CSS_SELECTOR, \"div.single-body p\")\n",
        "                    article_text = ' '.join([p.text for p in paragraphs])\n",
        "                except:\n",
        "                    article_text = \"\"\n",
        "\n",
        "                # Get date\n",
        "                try:\n",
        "                    date_str = driver.find_element(By.CSS_SELECTOR, \"time.date-published\").get_attribute('datetime')\n",
        "                    date_published = date_str.split('T')[0]  # Get just the date part\n",
        "                except:\n",
        "                    date_published = \"Unknown\"\n",
        "\n",
        "            elif source_domain == 'politifact.com':\n",
        "                # Extract title\n",
        "                title = driver.find_element(By.CSS_SELECTOR, \"h1.c-title\").text.strip()\n",
        "\n",
        "                # Get rating/verdict\n",
        "                try:\n",
        "                    verdict_img = driver.find_element(By.CSS_SELECTOR, \"div.c-image img.c-image__original\").get_attribute('alt')\n",
        "                    verdict = verdict_img if verdict_img else \"Unknown\"\n",
        "                except:\n",
        "                    try:\n",
        "                        verdict = driver.find_element(By.CSS_SELECTOR, \"div.meter\").get_attribute('class')\n",
        "                        # Clean up the verdict string\n",
        "                        if verdict:\n",
        "                            verdict_match = re.search(r'rating--(\\w+)', verdict)\n",
        "                            verdict = verdict_match.group(1) if verdict_match else \"Unknown\"\n",
        "                    except:\n",
        "                        verdict = \"Unknown\"\n",
        "\n",
        "                # Get claim\n",
        "                try:\n",
        "                    claim = driver.find_element(By.CSS_SELECTOR, \"div.statement__text\").text.strip()\n",
        "                except:\n",
        "                    claim = \"No claim specified\"\n",
        "\n",
        "                # Get article text\n",
        "                try:\n",
        "                    article_div = driver.find_element(By.CSS_SELECTOR, \"article.article__text\")\n",
        "                    paragraphs = article_div.find_elements(By.TAG_NAME, \"p\")\n",
        "                    article_text = ' '.join([p.text for p in paragraphs])\n",
        "                except:\n",
        "                    article_text = \"\"\n",
        "\n",
        "                # Get date\n",
        "                try:\n",
        "                    date_str = driver.find_element(By.CSS_SELECTOR, \"span.statement__date\").text.strip()\n",
        "                    date_published = date_str\n",
        "                except:\n",
        "                    date_published = \"Unknown\"\n",
        "\n",
        "            elif source_domain == 'factcheck.org':\n",
        "                # Extract title\n",
        "                title = driver.find_element(By.CSS_SELECTOR, \"h1.entry-title\").text.strip()\n",
        "\n",
        "                # Get claim (factcheck.org doesn't always have a clear claim section)\n",
        "                claim = \"See article for details\"\n",
        "\n",
        "                # Get article text\n",
        "                try:\n",
        "                    content_div = driver.find_element(By.CSS_SELECTOR, \"div.entry-content\")\n",
        "                    paragraphs = content_div.find_elements(By.TAG_NAME, \"p\")\n",
        "                    article_text = ' '.join([p.text for p in paragraphs])\n",
        "                except:\n",
        "                    article_text = \"\"\n",
        "\n",
        "                # For FactCheck.org, we don't have a clear verdict label, so we'll use keywords\n",
        "                if any(word in article_text.lower() for word in ['false', 'incorrect', 'misleading', 'fake']):\n",
        "                    verdict = \"False\"\n",
        "                elif any(word in article_text.lower() for word in ['partially true', 'partly true', 'half true']):\n",
        "                    verdict = \"Partially True\"\n",
        "                elif any(word in article_text.lower() for word in ['true', 'correct', 'accurate']):\n",
        "                    verdict = \"True\"\n",
        "                else:\n",
        "                    verdict = \"Unrated\"\n",
        "\n",
        "                # Get date\n",
        "                try:\n",
        "                    date_str = driver.find_element(By.CSS_SELECTOR, \"time.entry-date\").text.strip()\n",
        "                    date_published = date_str\n",
        "                except:\n",
        "                    date_published = \"Unknown\"\n",
        "\n",
        "            else:\n",
        "                # Generic extraction for unknown domains\n",
        "                title = driver.find_element(By.TAG_NAME, \"h1\").text.strip()\n",
        "                claim = \"See article for details\"\n",
        "                verdict = \"Unknown\"\n",
        "\n",
        "                # Get article text\n",
        "                try:\n",
        "                    paragraphs = driver.find_elements(By.TAG_NAME, \"p\")\n",
        "                    article_text = ' '.join([p.text for p in paragraphs])\n",
        "                except:\n",
        "                    article_text = \"\"\n",
        "\n",
        "                date_published = \"Unknown\"\n",
        "\n",
        "            return {\n",
        "                'title': title,\n",
        "                'claim': claim,\n",
        "                'verdict': verdict,\n",
        "                'article_text': article_text,\n",
        "                'url': url,\n",
        "                'date_published': date_published,\n",
        "                'date_crawled': datetime.now().strftime(\"%Y-%m-%d\"),\n",
        "                'source_domain': source_domain\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Selenium crawling of {url} failed: {str(e)}\")\n",
        "            # Fall back to requests method\n",
        "            return crawl_with_requests(url, source_domain)\n",
        "    else:\n",
        "        # If no driver is provided, use requests method\n",
        "        return crawl_with_requests(url, source_domain)\n",
        "\n",
        "# Main execution\n",
        "try:\n",
        "    # Try to set up Selenium WebDriver\n",
        "    driver = setup_driver()\n",
        "\n",
        "    # Get article URLs from each site\n",
        "    print(\"\\nFetching article URLs from fact-checking sites...\")\n",
        "    snopes_urls = get_snopes_article_urls(num_pages=2)\n",
        "    politifact_urls = get_politifact_article_urls(num_pages=2)\n",
        "    factcheck_urls = get_factcheck_article_urls(num_pages=2)\n",
        "\n",
        "    print(f\"\\nFound {len(snopes_urls)} Snopes articles\")\n",
        "    print(f\"Found {len(politifact_urls)} PolitiFact articles\")\n",
        "    print(f\"Found {len(factcheck_urls)} FactCheck.org articles\")\n",
        "\n",
        "    # Crawl each article URL\n",
        "    all_results = []\n",
        "\n",
        "    print(\"\\nCrawling Snopes articles...\")\n",
        "    for url in snopes_urls:\n",
        "        print(f\"Crawling: {url}\")\n",
        "        result = crawl_article(url, 'snopes.com', driver)\n",
        "        if result:\n",
        "            all_results.append(result)\n",
        "            print(f\"Successfully crawled: {result['title']}\")\n",
        "        time.sleep(random.uniform(1.0, 2.0))\n",
        "\n",
        "    print(\"\\nCrawling PolitiFact articles...\")\n",
        "    for url in politifact_urls:\n",
        "        print(f\"Crawling: {url}\")\n",
        "        result = crawl_article(url, 'politifact.com', driver)\n",
        "        if result:\n",
        "            all_results.append(result)\n",
        "            print(f\"Successfully crawled: {result['title']}\")\n",
        "        time.sleep(random.uniform(1.0, 2.0))\n",
        "\n",
        "    print(\"\\nCrawling FactCheck.org articles...\")\n",
        "    for url in factcheck_urls:\n",
        "        print(f\"Crawling: {url}\")\n",
        "        result = crawl_article(url, 'factcheck.org', driver)\n",
        "        if result:\n",
        "            all_results.append(result)\n",
        "            print(f\"Successfully crawled: {result['title']}\")\n",
        "        time.sleep(random.uniform(1.0, 2.0))\n",
        "\n",
        "    # Update the DataFrame\n",
        "    fake_news_db = pd.concat([fake_news_db, pd.DataFrame(all_results)], ignore_index=True)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during crawling process: {str(e)}\")\n",
        "\n",
        "finally:\n",
        "    # Close the WebDriver if it was initialized\n",
        "    if 'driver' in locals() and driver is not None:\n",
        "        driver.quit()\n",
        "\n",
        "# Display stats\n",
        "print(\"\\n--- Crawling Complete ---\")\n",
        "print(f\"Total articles collected: {len(fake_news_db)}\")\n",
        "print(f\"Articles by source:\")\n",
        "print(fake_news_db['source_domain'].value_counts())\n",
        "print(f\"Articles by verdict:\")\n",
        "print(fake_news_db['verdict'].value_counts())\n",
        "\n",
        "# Save to CSV\n",
        "fake_news_db.to_csv('fake_news_dataset.csv', index=False)\n",
        "print(\"\\nDataset saved to 'fake_news_dataset.csv'\")\n",
        "\n",
        "# Save to SQLite database\n",
        "import sqlite3\n",
        "conn = sqlite3.connect('fake_news.db')\n",
        "fake_news_db.to_sql('fact_checks', conn, if_exists='replace', index=False)\n",
        "conn.close()\n",
        "print(\"Dataset saved to SQLite database 'fake_news.db'\")\n",
        "\n",
        "# Display sample of the collected data\n",
        "print(\"\\n--- Sample Data ---\")\n",
        "fake_news_db.head()\n",
        "\n",
        "# Basic data analysis\n",
        "print(\"\\n--- Basic Data Analysis ---\")\n",
        "\n",
        "# Count by verdict type\n",
        "verdict_counts = fake_news_db['verdict'].value_counts()\n",
        "print(\"Verdict distribution:\")\n",
        "print(verdict_counts)\n",
        "\n",
        "# Most common words in false claims\n",
        "if len(fake_news_db) > 0 and 'article_text' in fake_news_db.columns:\n",
        "    print(\"\\nAnalyzing text content...\")\n",
        "\n",
        "    # Filter for false claims\n",
        "    false_claims = fake_news_db[fake_news_db['verdict'].str.contains('false|fake', case=False, na=False)]\n",
        "\n",
        "    if len(false_claims) > 0:\n",
        "        # Simple word count analysis\n",
        "        all_text = ' '.join(false_claims['article_text'].fillna(''))\n",
        "\n",
        "        # Remove common words\n",
        "        common_words = ['the', 'to', 'and', 'a', 'in', 'of', 'that', 'is', 'it', 'for', 'on', 'with', 'as', 'by', 'at']\n",
        "        word_counts = {}\n",
        "\n",
        "        for word in re.findall(r'\\b[a-zA-Z]{3,}\\b', all_text.lower()):\n",
        "            if word not in common_words:\n",
        "                word_counts[word] = word_counts.get(word, 0) + 1\n",
        "\n",
        "        # Display most common words\n",
        "        sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "        print(\"Most common words in false claims:\")\n",
        "        for word, count in sorted_words[:20]:\n",
        "            print(f\"  {word}: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "news_sources = [\n",
        "    # US newspapers\n",
        "    {\"name\": \"New York Times\", \"url\": \"https://www.nytimes.com\", \"section\": \"/world\"},\n",
        "    {\"name\": \"Washington Post\", \"url\": \"https://www.washingtonpost.com\", \"section\": \"/world\"},\n",
        "    {\"name\": \"Wall Street Journal\", \"url\": \"https://www.wsj.com\", \"section\": \"/news/world\"},\n",
        "    {\"name\": \"USA Today\", \"url\": \"https://www.usatoday.com\", \"section\": \"/news/world\"},\n",
        "    {\"name\": \"Los Angeles Times\", \"url\": \"https://www.latimes.com\", \"section\": \"/world-nation\"},\n",
        "\n",
        "    # UK newspapers\n",
        "    {\"name\": \"The Guardian\", \"url\": \"https://www.theguardian.com\", \"section\": \"/international\"},\n",
        "    {\"name\": \"BBC News\", \"url\": \"https://www.bbc.com\", \"section\": \"/news\"},\n",
        "    {\"name\": \"The Telegraph\", \"url\": \"https://www.telegraph.co.uk\", \"section\": \"/news/world/\"},\n",
        "    {\"name\": \"Financial Times\", \"url\": \"https://www.ft.com\", \"section\": \"/world\"},\n",
        "    {\"name\": \"The Independent\", \"url\": \"https://www.independent.co.uk\", \"section\": \"/news/world\"},\n",
        "\n",
        "    # Other international English newspapers\n",
        "    {\"name\": \"Al Jazeera\", \"url\": \"https://www.aljazeera.com\", \"section\": \"/news\"},\n",
        "    {\"name\": \"Reuters\", \"url\": \"https://www.reuters.com\", \"section\": \"/world\"},\n",
        "    {\"name\": \"CNN\", \"url\": \"https://www.cnn.com\", \"section\": \"/world\"},\n",
        "    {\"name\": \"The Times of India\", \"url\": \"https://timesofindia.indiatimes.com\", \"section\": \"/world\"},\n",
        "    {\"name\": \"South China Morning Post\", \"url\": \"https://www.scmp.com\", \"section\": \"/news/world\"},\n",
        "]\n",
        "\n",
        "# Function to extract article URLs from a newspaper homepage or section\n",
        "def get_article_urls(source):\n",
        "    article_urls = []\n",
        "    source_name = source[\"name\"]\n",
        "    base_url = source[\"url\"]\n",
        "    section_url = base_url + source[\"section\"]\n",
        "\n",
        "    print(f\"Fetching articles from {source_name} ({section_url})...\")\n",
        "\n",
        "    try:\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }\n",
        "        response = requests.get(section_url, headers=headers, timeout=10)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Find all links on the page\n",
        "            links = soup.find_all('a', href=True)\n",
        "\n",
        "            # Extract article URLs based on patterns\n",
        "            for link in links:\n",
        "                href = link['href']\n",
        "\n",
        "                # Skip navigation, category, tag links and javascript\n",
        "                if (href.startswith('#') or\n",
        "                    href.startswith('javascript:') or\n",
        "                    '/tag/' in href or\n",
        "                    '/category/' in href or\n",
        "                    '/section/' in href or\n",
        "                    '/author/' in href):\n",
        "                    continue\n",
        "\n",
        "                # Handle relative URLs\n",
        "                if href.startswith('/'):\n",
        "                    full_url = base_url + href\n",
        "                elif href.startswith('http'):\n",
        "                    full_url = href\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "                # Check if URL belongs to the same domain\n",
        "                if base_url.split('//')[1].split('/')[0] in full_url:\n",
        "                    # Make sure it looks like an article (contains year or article indicators)\n",
        "                    if (re.search(r'/202\\d/', full_url) or\n",
        "                        re.search(r'/\\d{4}/\\d{2}/\\d{2}/', full_url) or\n",
        "                        '/article/' in full_url or\n",
        "                        '/story/' in full_url or\n",
        "                        '/news/' in full_url):\n",
        "\n",
        "                        # Make sure URL isn't already in our list\n",
        "                        if full_url not in article_urls:\n",
        "                            article_urls.append(full_url)\n",
        "\n",
        "            print(f\"Found {len(article_urls)} potential article URLs from {source_name}\")\n",
        "\n",
        "        else:\n",
        "            print(f\"Failed to fetch {source_name} homepage, status code: {response.status_code}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching articles from {source_name}: {str(e)}\")\n",
        "\n",
        "    # Return a limited number of articles per source to avoid overwhelming\n",
        "    return article_urls[:10]  # Limit to 10 articles per source\n",
        "\n",
        "# Function to extract article content using newspaper3k library\n",
        "def extract_article_content(url, source_name):\n",
        "    print(f\"Extracting content from: {url}\")\n",
        "\n",
        "    try:\n",
        "        # Create an Article object\n",
        "        article = Article(url, config=config)\n",
        "\n",
        "        # Download and parse the article\n",
        "        article.download()\n",
        "        article.parse()\n",
        "\n",
        "        # Natural language processing for keywords and summary\n",
        "        try:\n",
        "            article.nlp()\n",
        "        except Exception as nlp_error:\n",
        "            print(f\"NLP processing error (non-critical): {str(nlp_error)}\")\n",
        "\n",
        "        # Extract the data\n",
        "        return {\n",
        "            'title': article.title,\n",
        "            'text': article.text,\n",
        "            'authors': ', '.join(article.authors) if article.authors else \"Unknown\",\n",
        "            'publish_date': article.publish_date.strftime('%Y-%m-%d') if article.publish_date else \"Unknown\",\n",
        "            'top_image': article.top_image,\n",
        "            'url': url,\n",
        "            'source': source_name,\n",
        "            'keywords': ', '.join(article.keywords) if hasattr(article, 'keywords') else \"\",\n",
        "            'summary': article.summary if hasattr(article, 'summary') else \"\",\n",
        "            'crawl_date': datetime.now().strftime(\"%Y-%m-%d\")\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting content from {url}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Function to attempt alternative extraction if newspaper3k fails\n",
        "def fallback_extract_article(url, source_name):\n",
        "    print(f\"Attempting fallback extraction for: {url}\")\n",
        "\n",
        "    try:\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }\n",
        "        response = requests.get(url, headers=headers, timeout=15)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Extract title\n",
        "            title = \"\"\n",
        "            title_tag = soup.find('h1')\n",
        "            if title_tag:\n",
        "                title = title_tag.text.strip()\n",
        "\n",
        "            # Extract content - look for article body, main content\n",
        "            content_selectors = [\n",
        "                'article',\n",
        "                'main',\n",
        "                'div[class*=\"article\"]',\n",
        "                'div[class*=\"content\"]',\n",
        "                'div[class*=\"story\"]',\n",
        "                'div[id*=\"article\"]',\n",
        "                'div[id*=\"content\"]',\n",
        "                'div[id*=\"story\"]'\n",
        "            ]\n",
        "\n",
        "            content = \"\"\n",
        "            for selector in content_selectors:\n",
        "                main_content = soup.select(selector)\n",
        "                if main_content:\n",
        "                    # Extract all paragraphs from this content area\n",
        "                    paragraphs = main_content[0].find_all('p')\n",
        "                    if paragraphs:\n",
        "                        content = ' '.join([p.text.strip() for p in paragraphs])\n",
        "                        break\n",
        "\n",
        "            # If still no content, just grab all paragraphs\n",
        "            if not content:\n",
        "                paragraphs = soup.find_all('p')\n",
        "                content = ' '.join([p.text.strip() for p in paragraphs[:20]])  # Limit to first 20 paragraphs\n",
        "\n",
        "            # Try to find publication date\n",
        "            date = \"Unknown\"\n",
        "            date_patterns = [\n",
        "                'time',\n",
        "                'span[class*=\"date\"]',\n",
        "                'div[class*=\"date\"]',\n",
        "                'p[class*=\"date\"]',\n",
        "                'meta[property=\"article:published_time\"]'\n",
        "            ]\n",
        "\n",
        "            for pattern in date_patterns:\n",
        "                date_element = soup.select_one(pattern)\n",
        "                if date_element:\n",
        "                    if date_element.name == 'meta' and date_element.get('content'):\n",
        "                        date = date_element['content'].split('T')[0]\n",
        "                    else:\n",
        "                        date = date_element.text.strip()\n",
        "                    break\n",
        "\n",
        "            # Basic keyword extraction\n",
        "            keywords = \"\"\n",
        "            keyword_meta = soup.find('meta', {'name': 'keywords'})\n",
        "            if keyword_meta and keyword_meta.get('content'):\n",
        "                keywords = keyword_meta['content']\n",
        "\n",
        "            return {\n",
        "                'title': title,\n",
        "                'text': content,\n",
        "                'authors': \"Unknown\",  # Difficult to reliably parse authors\n",
        "                'publish_date': date,\n",
        "                'top_image': \"\",  # Skip image extraction for fallback\n",
        "                'url': url,\n",
        "                'source': source_name,\n",
        "                'keywords': keywords,\n",
        "                'summary': \"\",  # Skip summary for fallback\n",
        "                'crawl_date': datetime.now().strftime(\"%Y-%m-%d\")\n",
        "            }\n",
        "\n",
        "        else:\n",
        "            print(f\"Failed fallback extraction, status code: {response.status_code}\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in fallback extraction for {url}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Main crawling function\n",
        "def crawl_news_sources(sources=news_sources, articles_per_source=5):\n",
        "    all_articles = []\n",
        "\n",
        "    for source in sources:\n",
        "        print(f\"\\n{'='*40}\")\n",
        "        print(f\"Crawling {source['name']}...\")\n",
        "        print(f\"{'='*40}\")\n",
        "\n",
        "        # Get article URLs\n",
        "        article_urls = get_article_urls(source)\n",
        "\n",
        "        # Limit to specified number of articles per source\n",
        "        article_urls = article_urls[:articles_per_source]\n",
        "\n",
        "        # Process each article\n",
        "        for url in article_urls:\n",
        "            print(f\"\\nProcessing: {url}\")\n",
        "\n",
        "            # Try to extract with newspaper3k\n",
        "            article_data = extract_article_content(url, source['name'])\n",
        "\n",
        "            # If failed, try fallback method\n",
        "            if article_data is None or not article_data.get('text'):\n",
        "                print(\"Primary extraction failed, attempting fallback...\")\n",
        "                article_data = fallback_extract_article(url, source['name'])\n",
        "\n",
        "            # If successful, add to results\n",
        "            if article_data and article_data.get('title') and article_data.get('text'):\n",
        "                all_articles.append(article_data)\n",
        "                print(f\"Successfully extracted: {article_data['title']}\")\n",
        "            else:\n",
        "                print(f\"Failed to extract content from {url}\")\n",
        "\n",
        "            # Polite delay\n",
        "            time.sleep(random.uniform(1.0, 3.0))\n",
        "\n",
        "    return all_articles\n",
        "\n",
        "# Function to analyze news articles for potential indicators of fake news\n",
        "def analyze_news_content(df):\n",
        "    # Add analysis columns\n",
        "    df['text_length'] = df['text'].apply(lambda x: len(x.split()))\n",
        "    df['clickbait_score'] = 0\n",
        "    df['emotional_score'] = 0\n",
        "    df['sensational_score'] = 0\n",
        "\n",
        "    # Clickbait title indicators\n",
        "    clickbait_patterns = [\n",
        "        r'(?i)you won\\'t believe',\n",
        "        r'(?i)shocking',\n",
        "        r'(?i)amazing',\n",
        "        r'(?i)incredible',\n",
        "        r'(?i)mind[-\\s]?blowing',\n",
        "        r'(?i)this is why',\n",
        "        r'(?i)secret',\n",
        "        r'(?i)surprising',\n",
        "        r'(?i)unbelievable',\n",
        "        r'(?i)\\d+\\s+(?:things|ways|reasons|facts|tricks|ideas|tips)',\n",
        "        r'(?i)what happens next',\n",
        "        r'(?i)this is what',\n",
        "        r'(?i)must see',\n",
        "        r'(?i)here\\'s why',\n",
        "        r'(?i)this is how'\n",
        "    ]\n",
        "\n",
        "    # Emotional language indicators\n",
        "    emotional_words = [\n",
        "        'outrage', 'angry', 'fury', 'furious', 'panic', 'terrified', 'terrifying',\n",
        "        'horrific', 'devastating', 'tragic', 'heartbreaking', 'shocking', 'alarming',\n",
        "        'disaster', 'crisis', 'catastrophe', 'emergency', 'scandal', 'bombshell', 'slams',\n",
        "        'blasts', 'condemns', 'rips', 'destroys', 'annihilates'\n",
        "    ]\n",
        "\n",
        "    # Sensational phrases\n",
        "    sensational_phrases = [\n",
        "        'breaking news', 'exclusive', 'sources say', 'anonymous sources',\n",
        "        'according to sources', 'experts say', 'scientists claim', 'doctors reveal',\n",
        "        'studies show', 'research proves', 'government officials', 'officials say'\n",
        "    ]\n",
        "\n",
        "    # Analyze each article\n",
        "    for idx, row in df.iterrows():\n",
        "        title = row['title']\n",
        "        text = row['text']\n",
        "\n",
        "        # Check for clickbait patterns in title\n",
        "        clickbait_count = sum(1 for pattern in clickbait_patterns if re.search(pattern, title))\n",
        "        df.at[idx, 'clickbait_score'] = min(clickbait_count * 10, 100)  # Scale from 0-100\n",
        "\n",
        "        # Check for emotional language\n",
        "        text_lower = text.lower()\n",
        "        emotional_count = sum(1 for word in emotional_words if word in text_lower)\n",
        "        df.at[idx, 'emotional_score'] = min((emotional_count / max(len(text.split()) / 100, 1)) * 100, 100)\n",
        "\n",
        "        # Check for sensational phrases\n",
        "        sensational_count = sum(1 for phrase in sensational_phrases if phrase in text_lower)\n",
        "        df.at[idx, 'sensational_score'] = min((sensational_count / max(len(text.split()) / 200, 1)) * 100, 100)\n",
        "\n",
        "    # Calculate overall credibility score (inverse of problematic indicators)\n",
        "    df['potential_fake_news_score'] = (df['clickbait_score'] + df['emotional_score'] + df['sensational_score']) / 3\n",
        "\n",
        "    return df\n",
        "\n",
        "# Main execution\n",
        "print(\"Starting to crawl English language newspapers...\\n\")\n",
        "\n",
        "try:\n",
        "    # Execute the crawling\n",
        "    articles = crawl_news_sources(articles_per_source=5)  # Adjust the number as needed\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    news_db = pd.DataFrame(articles)\n",
        "\n",
        "    # Basic analysis of the content\n",
        "    if len(news_db) > 0:\n",
        "        news_db = analyze_news_content(news_db)\n",
        "\n",
        "    # Display stats\n",
        "    print(\"\\n--- Crawling Complete ---\")\n",
        "    print(f\"Total articles collected: {len(news_db)}\")\n",
        "    print(f\"Articles by source:\")\n",
        "    print(news_db['source'].value_counts())\n",
        "\n",
        "    # Save to CSV\n",
        "    news_db.to_csv('newspaper_dataset.csv', index=False)\n",
        "    print(\"\\nDataset saved to 'newspaper_dataset.csv'\")\n",
        "\n",
        "    # Save to SQLite database\n",
        "    conn = sqlite3.connect('newspaper_articles.db')\n",
        "    news_db.to_sql('articles', conn, if_exists='replace', index=False)\n",
        "    conn.close()\n",
        "    print(\"Dataset saved to SQLite database 'newspaper_articles.db'\")\n",
        "\n",
        "    # Display sample of the collected data\n",
        "    print(\"\\n--- Sample Data ---\")\n",
        "    sample_columns = ['title', 'source', 'publish_date', 'authors', 'potential_fake_news_score']\n",
        "    print(news_db[sample_columns].head())\n",
        "\n",
        "    # Basic analysis results\n",
        "    if 'potential_fake_news_score' in news_db.columns:\n",
        "        print(\"\\n--- Fake News Analysis Results ---\")\n",
        "        print(f\"Average potential fake news score: {news_db['potential_fake_news_score'].mean():.2f}/100\")\n",
        "        print(\"\\nArticles with highest fake news indicators:\")\n",
        "        high_score_articles = news_db.sort_values('potential_fake_news_score', ascending=False).head(5)\n",
        "        for idx, row in high_score_articles.iterrows():\n",
        "            print(f\"- {row['title']} ({row['source']}) - Score: {row['potential_fake_news_score']:.2f}\")\n",
        "\n",
        "        print(\"\\nArticles with lowest fake news indicators:\")\n",
        "        low_score_articles = news_db.sort_values('potential_fake_news_score').head(5)\n",
        "        for idx, row in low_score_articles.iterrows():\n",
        "            print(f\"- {row['title']} ({row['source']}) - Score: {row['potential_fake_news_score']:.2f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during execution: {str(e)}\")\n",
        "\n",
        "print(\"\\nCrawling process completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nh7kGqtH5vmT",
        "outputId": "5d05056b-6b4c-4c12-d94c-bf0c26c2811b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting to crawl English language newspapers...\n",
            "\n",
            "\n",
            "========================================\n",
            "Crawling New York Times...\n",
            "========================================\n",
            "Fetching articles from New York Times (https://www.nytimes.com/world)...\n",
            "Found 0 potential article URLs from New York Times\n",
            "\n",
            "========================================\n",
            "Crawling Washington Post...\n",
            "========================================\n",
            "Fetching articles from Washington Post (https://www.washingtonpost.com/world)...\n",
            "Error fetching articles from Washington Post: HTTPSConnectionPool(host='www.washingtonpost.com', port=443): Read timed out. (read timeout=10)\n",
            "\n",
            "========================================\n",
            "Crawling Wall Street Journal...\n",
            "========================================\n",
            "Fetching articles from Wall Street Journal (https://www.wsj.com/news/world)...\n",
            "Failed to fetch Wall Street Journal homepage, status code: 401\n",
            "\n",
            "========================================\n",
            "Crawling USA Today...\n",
            "========================================\n",
            "Fetching articles from USA Today (https://www.usatoday.com/news/world)...\n",
            "Found 20 potential article URLs from USA Today\n",
            "\n",
            "Processing: https://www.usatoday.com/news/nation/\n",
            "Extracting content from: https://www.usatoday.com/news/nation/\n",
            "Error extracting content from https://www.usatoday.com/news/nation/: name 'Article' is not defined\n",
            "Primary extraction failed, attempting fallback...\n",
            "Attempting fallback extraction for: https://www.usatoday.com/news/nation/\n",
            "Failed to extract content from https://www.usatoday.com/news/nation/\n",
            "\n",
            "Processing: https://www.usatoday.com/news/politics/\n",
            "Extracting content from: https://www.usatoday.com/news/politics/\n",
            "Error extracting content from https://www.usatoday.com/news/politics/: name 'Article' is not defined\n",
            "Primary extraction failed, attempting fallback...\n",
            "Attempting fallback extraction for: https://www.usatoday.com/news/politics/\n",
            "Failed to extract content from https://www.usatoday.com/news/politics/\n",
            "\n",
            "Processing: https://www.usatoday.com/story/news/world/2025/04/29/polar-bear-svalbard-norway-video/83342602007/\n",
            "Extracting content from: https://www.usatoday.com/story/news/world/2025/04/29/polar-bear-svalbard-norway-video/83342602007/\n",
            "Error extracting content from https://www.usatoday.com/story/news/world/2025/04/29/polar-bear-svalbard-norway-video/83342602007/: name 'Article' is not defined\n",
            "Primary extraction failed, attempting fallback...\n",
            "Attempting fallback extraction for: https://www.usatoday.com/story/news/world/2025/04/29/polar-bear-svalbard-norway-video/83342602007/\n",
            "Successfully extracted: Watch terrifying moment as man narrowly escapes polar bear as it charges at him in Norway\n",
            "\n",
            "Processing: https://www.usatoday.com/picture-gallery/news/world/2025/04/29/conclave-see-new-pope-elected/83343540007/\n",
            "Extracting content from: https://www.usatoday.com/picture-gallery/news/world/2025/04/29/conclave-see-new-pope-elected/83343540007/\n",
            "Error extracting content from https://www.usatoday.com/picture-gallery/news/world/2025/04/29/conclave-see-new-pope-elected/83343540007/: name 'Article' is not defined\n",
            "Primary extraction failed, attempting fallback...\n",
            "Attempting fallback extraction for: https://www.usatoday.com/picture-gallery/news/world/2025/04/29/conclave-see-new-pope-elected/83343540007/\n",
            "Failed to extract content from https://www.usatoday.com/picture-gallery/news/world/2025/04/29/conclave-see-new-pope-elected/83343540007/\n",
            "\n",
            "Processing: https://www.usatoday.com/story/news/world/2025/04/29/sistine-chapel-conclave-new-pope-francis-michelangelo/83342420007/\n",
            "Extracting content from: https://www.usatoday.com/story/news/world/2025/04/29/sistine-chapel-conclave-new-pope-francis-michelangelo/83342420007/\n",
            "Error extracting content from https://www.usatoday.com/story/news/world/2025/04/29/sistine-chapel-conclave-new-pope-francis-michelangelo/83342420007/: name 'Article' is not defined\n",
            "Primary extraction failed, attempting fallback...\n",
            "Attempting fallback extraction for: https://www.usatoday.com/story/news/world/2025/04/29/sistine-chapel-conclave-new-pope-francis-michelangelo/83342420007/\n",
            "Successfully extracted: What to know about the Sistine Chapel, site of the conclave to pick a new pope\n",
            "\n",
            "========================================\n",
            "Crawling Los Angeles Times...\n",
            "========================================\n",
            "Fetching articles from Los Angeles Times (https://www.latimes.com/world-nation)...\n",
            "Found 44 potential article URLs from Los Angeles Times\n",
            "\n",
            "Processing: https://www.latimes.com/lifestyle/image/story/2022-09-13/trend-analysis\n",
            "Extracting content from: https://www.latimes.com/lifestyle/image/story/2022-09-13/trend-analysis\n",
            "Error extracting content from https://www.latimes.com/lifestyle/image/story/2022-09-13/trend-analysis: name 'Article' is not defined\n",
            "Primary extraction failed, attempting fallback...\n",
            "Attempting fallback extraction for: https://www.latimes.com/lifestyle/image/story/2022-09-13/trend-analysis\n",
            "Successfully extracted: Art & Culture\n",
            "\n",
            "Processing: https://www.latimes.com/lifestyle/image/story/2022-09-13/los-intelligentsia\n",
            "Extracting content from: https://www.latimes.com/lifestyle/image/story/2022-09-13/los-intelligentsia\n",
            "Error extracting content from https://www.latimes.com/lifestyle/image/story/2022-09-13/los-intelligentsia: name 'Article' is not defined\n",
            "Primary extraction failed, attempting fallback...\n",
            "Attempting fallback extraction for: https://www.latimes.com/lifestyle/image/story/2022-09-13/los-intelligentsia\n",
            "Successfully extracted: Conversations\n",
            "\n",
            "Processing: https://www.latimes.com/lifestyle/image/story/2022-09-13/drip-index\n",
            "Extracting content from: https://www.latimes.com/lifestyle/image/story/2022-09-13/drip-index\n",
            "Error extracting content from https://www.latimes.com/lifestyle/image/story/2022-09-13/drip-index: name 'Article' is not defined\n",
            "Primary extraction failed, attempting fallback...\n",
            "Attempting fallback extraction for: https://www.latimes.com/lifestyle/image/story/2022-09-13/drip-index\n",
            "Successfully extracted: Your L.A. event guide\n",
            "\n",
            "Processing: https://www.latimes.com/lifestyle/image/story/2022-09-13/coveted\n",
            "Extracting content from: https://www.latimes.com/lifestyle/image/story/2022-09-13/coveted\n",
            "Error extracting content from https://www.latimes.com/lifestyle/image/story/2022-09-13/coveted: name 'Article' is not defined\n",
            "Primary extraction failed, attempting fallback...\n",
            "Attempting fallback extraction for: https://www.latimes.com/lifestyle/image/story/2022-09-13/coveted\n",
            "Successfully extracted: A curated fashion and beauty guide\n",
            "\n",
            "Processing: https://www.latimes.com/lifestyle/image/story/2022-09-13/styling-myself\n",
            "Extracting content from: https://www.latimes.com/lifestyle/image/story/2022-09-13/styling-myself\n",
            "Error extracting content from https://www.latimes.com/lifestyle/image/story/2022-09-13/styling-myself: name 'Article' is not defined\n",
            "Primary extraction failed, attempting fallback...\n",
            "Attempting fallback extraction for: https://www.latimes.com/lifestyle/image/story/2022-09-13/styling-myself\n",
            "Successfully extracted: Styling Myself\n",
            "\n",
            "========================================\n",
            "Crawling The Guardian...\n",
            "========================================\n",
            "Fetching articles from The Guardian (https://www.theguardian.com/international)...\n",
            "Found 129 potential article URLs from The Guardian\n",
            "\n",
            "Processing: https://www.theguardian.com/world/2025/apr/29/canada-election-result-liberal-win-mark-carney-anti-trump\n",
            "Extracting content from: https://www.theguardian.com/world/2025/apr/29/canada-election-result-liberal-win-mark-carney-anti-trump\n",
            "Error extracting content from https://www.theguardian.com/world/2025/apr/29/canada-election-result-liberal-win-mark-carney-anti-trump: name 'Article' is not defined\n",
            "Primary extraction failed, attempting fallback...\n",
            "Attempting fallback extraction for: https://www.theguardian.com/world/2025/apr/29/canada-election-result-liberal-win-mark-carney-anti-trump\n",
            "Successfully extracted: ‘Trump wanted to break us’, says Carney as Liberals triumph in Canadian election\n",
            "\n",
            "Processing: https://www.theguardian.com/world/2025/apr/29/canada-election-pierre-poilievre-seat-parliament\n",
            "Extracting content from: https://www.theguardian.com/world/2025/apr/29/canada-election-pierre-poilievre-seat-parliament\n",
            "Error extracting content from https://www.theguardian.com/world/2025/apr/29/canada-election-pierre-poilievre-seat-parliament: name 'Article' is not defined\n",
            "Primary extraction failed, attempting fallback...\n",
            "Attempting fallback extraction for: https://www.theguardian.com/world/2025/apr/29/canada-election-pierre-poilievre-seat-parliament\n",
            "Successfully extracted: Canada election: Conservative leader Pierre Poilievre loses seat he held since 2004\n",
            "\n",
            "Processing: https://www.theguardian.com/world/2025/apr/29/triumph-mark-carney-what-happened-canada-election-and-what-will-it-mean\n",
            "Extracting content from: https://www.theguardian.com/world/2025/apr/29/triumph-mark-carney-what-happened-canada-election-and-what-will-it-mean\n",
            "Error extracting content from https://www.theguardian.com/world/2025/apr/29/triumph-mark-carney-what-happened-canada-election-and-what-will-it-mean: name 'Article' is not defined\n",
            "Primary extraction failed, attempting fallback...\n",
            "Attempting fallback extraction for: https://www.theguardian.com/world/2025/apr/29/triumph-mark-carney-what-happened-canada-election-and-what-will-it-mean\n",
            "Successfully extracted: Triumph for Carney: what happened in Canada’s election, and what will it mean?\n",
            "\n",
            "Processing: https://www.theguardian.com/world/2025/apr/29/mark-carney-us-canada-relations\n",
            "Extracting content from: https://www.theguardian.com/world/2025/apr/29/mark-carney-us-canada-relations\n",
            "Error extracting content from https://www.theguardian.com/world/2025/apr/29/mark-carney-us-canada-relations: name 'Article' is not defined\n",
            "Primary extraction failed, attempting fallback...\n",
            "Attempting fallback extraction for: https://www.theguardian.com/world/2025/apr/29/mark-carney-us-canada-relations\n",
            "Successfully extracted: Carney gave a eulogy for Canada’s old relationship with the US. Now he must redefine it\n",
            "\n",
            "Processing: https://www.theguardian.com/technology/2025/apr/29/amazon-trump-tariff-costs\n",
            "Extracting content from: https://www.theguardian.com/technology/2025/apr/29/amazon-trump-tariff-costs\n",
            "Error extracting content from https://www.theguardian.com/technology/2025/apr/29/amazon-trump-tariff-costs: name 'Article' is not defined\n",
            "Primary extraction failed, attempting fallback...\n",
            "Attempting fallback extraction for: https://www.theguardian.com/technology/2025/apr/29/amazon-trump-tariff-costs\n",
            "Successfully extracted: White House calls Amazon ‘hostile’ for reportedly planning to list tariff costs\n",
            "\n",
            "========================================\n",
            "Crawling BBC News...\n",
            "========================================\n",
            "Fetching articles from BBC News (https://www.bbc.com/news)...\n",
            "Found 57 potential article URLs from BBC News\n",
            "\n",
            "Processing: https://www.bbc.com/news/topics/c2vdnvdg6xxt\n",
            "Extracting content from: https://www.bbc.com/news/topics/c2vdnvdg6xxt\n",
            "Error extracting content from https://www.bbc.com/news/topics/c2vdnvdg6xxt: name 'Article' is not defined\n",
            "Primary extraction failed, attempting fallback...\n",
            "Attempting fallback extraction for: https://www.bbc.com/news/topics/c2vdnvdg6xxt\n",
            "Successfully extracted: NewsNews\n",
            "\n",
            "Processing: https://www.bbc.com/news/war-in-ukraine\n",
            "Extracting content from: https://www.bbc.com/news/war-in-ukraine\n",
            "Error extracting content from https://www.bbc.com/news/war-in-ukraine: name 'Article' is not defined\n",
            "Primary extraction failed, attempting fallback...\n",
            "Attempting fallback extraction for: https://www.bbc.com/news/war-in-ukraine\n",
            "Successfully extracted: NewsNews\n",
            "\n",
            "Processing: https://www.bbc.com/news/us-canada\n",
            "Extracting content from: https://www.bbc.com/news/us-canada\n",
            "Error extracting content from https://www.bbc.com/news/us-canada: name 'Article' is not defined\n",
            "Primary extraction failed, attempting fallback...\n",
            "Attempting fallback extraction for: https://www.bbc.com/news/us-canada\n",
            "Successfully extracted: NewsNews\n",
            "\n",
            "Processing: https://www.bbc.com/news/uk\n",
            "Extracting content from: https://www.bbc.com/news/uk\n",
            "Error extracting content from https://www.bbc.com/news/uk: name 'Article' is not defined\n",
            "Primary extraction failed, attempting fallback...\n",
            "Attempting fallback extraction for: https://www.bbc.com/news/uk\n",
            "Successfully extracted: NewsNews\n",
            "\n",
            "Processing: https://www.bbc.com/news/world/africa\n",
            "Extracting content from: https://www.bbc.com/news/world/africa\n",
            "Error extracting content from https://www.bbc.com/news/world/africa: name 'Article' is not defined\n",
            "Primary extraction failed, attempting fallback...\n",
            "Attempting fallback extraction for: https://www.bbc.com/news/world/africa\n",
            "Successfully extracted: NewsNews\n",
            "\n",
            "========================================\n",
            "Crawling The Telegraph...\n",
            "========================================\n",
            "Fetching articles from The Telegraph (https://www.telegraph.co.uk/news/world/)...\n",
            "Failed to fetch The Telegraph homepage, status code: 403\n",
            "\n",
            "========================================\n",
            "Crawling Financial Times...\n",
            "========================================\n",
            "Fetching articles from Financial Times (https://www.ft.com/world)...\n",
            "Found 0 potential article URLs from Financial Times\n",
            "\n",
            "========================================\n",
            "Crawling The Independent...\n",
            "========================================\n",
            "Fetching articles from The Independent (https://www.independent.co.uk/news/world)...\n",
            "Found 102 potential article URLs from The Independent\n",
            "\n",
            "Processing: https://www.independent.co.uk/news/world/americas\n",
            "Extracting content from: https://www.independent.co.uk/news/world/americas\n",
            "Error extracting content from https://www.independent.co.uk/news/world/americas: name 'Article' is not defined\n",
            "Primary extraction failed, attempting fallback...\n",
            "Attempting fallback extraction for: https://www.independent.co.uk/news/world/americas\n",
            "Successfully extracted: Americas\n",
            "\n",
            "Processing: https://www.independent.co.uk/news/uk\n",
            "Extracting content from: https://www.independent.co.uk/news/uk\n",
            "Error extracting content from https://www.independent.co.uk/news/uk: name 'Article' is not defined\n",
            "Primary extraction failed, attempting fallback...\n",
            "Attempting fallback extraction for: https://www.independent.co.uk/news/uk\n",
            "Successfully extracted: UK\n",
            "\n",
            "Processing: https://www.independent.co.uk/news/world/europe\n",
            "Extracting content from: https://www.independent.co.uk/news/world/europe\n",
            "Error extracting content from https://www.independent.co.uk/news/world/europe: name 'Article' is not defined\n",
            "Primary extraction failed, attempting fallback...\n",
            "Attempting fallback extraction for: https://www.independent.co.uk/news/world/europe\n",
            "Successfully extracted: Europe\n",
            "\n",
            "Processing: https://www.independent.co.uk/news/world\n",
            "Extracting content from: https://www.independent.co.uk/news/world\n",
            "Error extracting content from https://www.independent.co.uk/news/world: name 'Article' is not defined\n",
            "Primary extraction failed, attempting fallback...\n",
            "Attempting fallback extraction for: https://www.independent.co.uk/news/world\n",
            "Successfully extracted: World\n",
            "\n",
            "Processing: https://www.independent.co.uk/news/world/americas/us-politics\n",
            "Extracting content from: https://www.independent.co.uk/news/world/americas/us-politics\n",
            "Error extracting content from https://www.independent.co.uk/news/world/americas/us-politics: name 'Article' is not defined\n",
            "Primary extraction failed, attempting fallback...\n",
            "Attempting fallback extraction for: https://www.independent.co.uk/news/world/americas/us-politics\n",
            "Successfully extracted: US politics\n",
            "\n",
            "========================================\n",
            "Crawling Al Jazeera...\n",
            "========================================\n",
            "Fetching articles from Al Jazeera (https://www.aljazeera.com/news)...\n",
            "Found 15 potential article URLs from Al Jazeera\n",
            "\n",
            "Processing: https://www.aljazeera.com/news/\n",
            "Extracting content from: https://www.aljazeera.com/news/\n",
            "Error extracting content from https://www.aljazeera.com/news/: name 'Article' is not defined\n",
            "Primary extraction failed, attempting fallback...\n",
            "Attempting fallback extraction for: https://www.aljazeera.com/news/\n",
            "Successfully extracted: News\n",
            "\n",
            "Processing: https://www.aljazeera.com/news/2025/4/29/at-least-three-killed-in-sweden-shooting-police\n",
            "Extracting content from: https://www.aljazeera.com/news/2025/4/29/at-least-three-killed-in-sweden-shooting-police\n",
            "Error extracting content from https://www.aljazeera.com/news/2025/4/29/at-least-three-killed-in-sweden-shooting-police: name 'Article' is not defined\n",
            "Primary extraction failed, attempting fallback...\n",
            "Attempting fallback extraction for: https://www.aljazeera.com/news/2025/4/29/at-least-three-killed-in-sweden-shooting-police\n",
            "Successfully extracted: At least three killed in Sweden shooting: Police\n",
            "\n",
            "Processing: https://www.aljazeera.com/news/2025/4/29/at-least-22-people-killed-in-restaurant-fire-in-northeast-china\n",
            "Extracting content from: https://www.aljazeera.com/news/2025/4/29/at-least-22-people-killed-in-restaurant-fire-in-northeast-china\n",
            "Error extracting content from https://www.aljazeera.com/news/2025/4/29/at-least-22-people-killed-in-restaurant-fire-in-northeast-china: name 'Article' is not defined\n",
            "Primary extraction failed, attempting fallback...\n",
            "Attempting fallback extraction for: https://www.aljazeera.com/news/2025/4/29/at-least-22-people-killed-in-restaurant-fire-in-northeast-china\n",
            "Successfully extracted: At least 22 people killed in restaurant fire in northeast China\n",
            "\n",
            "Processing: https://www.aljazeera.com/news/2025/4/29/israel-releases-gaza-paramedic-who-survived-deadly-attack-on-health-workers\n",
            "Extracting content from: https://www.aljazeera.com/news/2025/4/29/israel-releases-gaza-paramedic-who-survived-deadly-attack-on-health-workers\n",
            "Error extracting content from https://www.aljazeera.com/news/2025/4/29/israel-releases-gaza-paramedic-who-survived-deadly-attack-on-health-workers: name 'Article' is not defined\n",
            "Primary extraction failed, attempting fallback...\n",
            "Attempting fallback extraction for: https://www.aljazeera.com/news/2025/4/29/israel-releases-gaza-paramedic-who-survived-deadly-attack-on-health-workers\n",
            "Successfully extracted: Israel releases Gaza paramedic who survived deadly attack on health workers\n",
            "\n",
            "Processing: https://www.aljazeera.com/economy/2025/4/29/how-much-revenue-has-the-us-earned-from-trumps-tariffs\n",
            "Extracting content from: https://www.aljazeera.com/economy/2025/4/29/how-much-revenue-has-the-us-earned-from-trumps-tariffs\n",
            "Error extracting content from https://www.aljazeera.com/economy/2025/4/29/how-much-revenue-has-the-us-earned-from-trumps-tariffs: name 'Article' is not defined\n",
            "Primary extraction failed, attempting fallback...\n",
            "Attempting fallback extraction for: https://www.aljazeera.com/economy/2025/4/29/how-much-revenue-has-the-us-earned-from-trumps-tariffs\n",
            "Successfully extracted: How much revenue has the US earned from Trump’s tariffs?\n",
            "\n",
            "========================================\n",
            "Crawling Reuters...\n",
            "========================================\n",
            "Fetching articles from Reuters (https://www.reuters.com/world)...\n",
            "Failed to fetch Reuters homepage, status code: 401\n",
            "\n",
            "========================================\n",
            "Crawling CNN...\n",
            "========================================\n",
            "Fetching articles from CNN (https://www.cnn.com/world)...\n",
            "Found 82 potential article URLs from CNN\n",
            "\n",
            "Processing: https://www.cnn.com/2025/04/29/europe/sweden-uppsala-shooting-intl-latam/index.html\n",
            "Extracting content from: https://www.cnn.com/2025/04/29/europe/sweden-uppsala-shooting-intl-latam/index.html\n",
            "Error extracting content from https://www.cnn.com/2025/04/29/europe/sweden-uppsala-shooting-intl-latam/index.html: name 'Article' is not defined\n",
            "Primary extraction failed, attempting fallback...\n",
            "Attempting fallback extraction for: https://www.cnn.com/2025/04/29/europe/sweden-uppsala-shooting-intl-latam/index.html\n",
            "Successfully extracted: Three dead after shooting in Sweden, police say\n",
            "\n",
            "Processing: https://www.cnn.com/2025/04/28/americas/canada-election-results-carney-poilievre-intl-hnk/index.html\n",
            "Extracting content from: https://www.cnn.com/2025/04/28/americas/canada-election-results-carney-poilievre-intl-hnk/index.html\n",
            "Error extracting content from https://www.cnn.com/2025/04/28/americas/canada-election-results-carney-poilievre-intl-hnk/index.html: name 'Article' is not defined\n",
            "Primary extraction failed, attempting fallback...\n",
            "Attempting fallback extraction for: https://www.cnn.com/2025/04/28/americas/canada-election-results-carney-poilievre-intl-hnk/index.html\n",
            "Successfully extracted: Canada will ‘never’ yield to Trump’s threats as Prime Minister Carney declares election victory\n",
            "\n",
            "Processing: https://www.cnn.com/2025/04/29/middleeast/overheard-sara-netanyahu-fewer-24-gaza-hostages-alive-intl/index.html\n",
            "Extracting content from: https://www.cnn.com/2025/04/29/middleeast/overheard-sara-netanyahu-fewer-24-gaza-hostages-alive-intl/index.html\n",
            "Error extracting content from https://www.cnn.com/2025/04/29/middleeast/overheard-sara-netanyahu-fewer-24-gaza-hostages-alive-intl/index.html: name 'Article' is not defined\n",
            "Primary extraction failed, attempting fallback...\n",
            "Attempting fallback extraction for: https://www.cnn.com/2025/04/29/middleeast/overheard-sara-netanyahu-fewer-24-gaza-hostages-alive-intl/index.html\n",
            "Successfully extracted: Overheard on mic, Netanyahu’s wife says ‘fewer’ than 24 hostages are still alive in Gaza\n",
            "\n",
            "Processing: https://www.cnn.com/2025/04/29/middleeast/palestinian-journalist-israel-detained-intl/index.html\n",
            "Extracting content from: https://www.cnn.com/2025/04/29/middleeast/palestinian-journalist-israel-detained-intl/index.html\n",
            "Error extracting content from https://www.cnn.com/2025/04/29/middleeast/palestinian-journalist-israel-detained-intl/index.html: name 'Article' is not defined\n",
            "Primary extraction failed, attempting fallback...\n",
            "Attempting fallback extraction for: https://www.cnn.com/2025/04/29/middleeast/palestinian-journalist-israel-detained-intl/index.html\n",
            "Successfully extracted: Israeli forces detain prominent Palestinian journalist in early morning West Bank raid\n",
            "\n",
            "Processing: https://www.cnn.com/2025/04/28/middleeast/ronen-bar-netanyahu-intl-latam/index.html\n",
            "Extracting content from: https://www.cnn.com/2025/04/28/middleeast/ronen-bar-netanyahu-intl-latam/index.html\n",
            "Error extracting content from https://www.cnn.com/2025/04/28/middleeast/ronen-bar-netanyahu-intl-latam/index.html: name 'Article' is not defined\n",
            "Primary extraction failed, attempting fallback...\n",
            "Attempting fallback extraction for: https://www.cnn.com/2025/04/28/middleeast/ronen-bar-netanyahu-intl-latam/index.html\n",
            "Successfully extracted: Israeli security chief says he’ll step down June 15, facing pressure from Netanyahu\n",
            "\n",
            "========================================\n",
            "Crawling The Times of India...\n",
            "========================================\n",
            "Fetching articles from The Times of India (https://timesofindia.indiatimes.com/world)...\n",
            "Found 33 potential article URLs from The Times of India\n",
            "\n",
            "Processing: https://timesofindia.indiatimes.com/videos/news/india-pakistan-war-looms-armies-exchange-fire-in-jammu-for-the-first-time-after-pahalgam-attack/videoshow/120738837.cms\n",
            "Extracting content from: https://timesofindia.indiatimes.com/videos/news/india-pakistan-war-looms-armies-exchange-fire-in-jammu-for-the-first-time-after-pahalgam-attack/videoshow/120738837.cms\n",
            "Error extracting content from https://timesofindia.indiatimes.com/videos/news/india-pakistan-war-looms-armies-exchange-fire-in-jammu-for-the-first-time-after-pahalgam-attack/videoshow/120738837.cms: name 'Article' is not defined\n",
            "Primary extraction failed, attempting fallback...\n",
            "Attempting fallback extraction for: https://timesofindia.indiatimes.com/videos/news/india-pakistan-war-looms-armies-exchange-fire-in-jammu-for-the-first-time-after-pahalgam-attack/videoshow/120738837.cms\n",
            "Failed to extract content from https://timesofindia.indiatimes.com/videos/news/india-pakistan-war-looms-armies-exchange-fire-in-jammu-for-the-first-time-after-pahalgam-attack/videoshow/120738837.cms\n",
            "\n",
            "Processing: https://timesofindia.indiatimes.com/education/news/cisce-to-announce-icse-isc-2025-results-on-this-date-heres-how-to-check/articleshow/120734195.cms\n",
            "Extracting content from: https://timesofindia.indiatimes.com/education/news/cisce-to-announce-icse-isc-2025-results-on-this-date-heres-how-to-check/articleshow/120734195.cms\n",
            "Error extracting content from https://timesofindia.indiatimes.com/education/news/cisce-to-announce-icse-isc-2025-results-on-this-date-heres-how-to-check/articleshow/120734195.cms: name 'Article' is not defined\n",
            "Primary extraction failed, attempting fallback...\n",
            "Attempting fallback extraction for: https://timesofindia.indiatimes.com/education/news/cisce-to-announce-icse-isc-2025-results-on-this-date-heres-how-to-check/articleshow/120734195.cms\n",
            "Successfully extracted: CISCE to announce ICSE & ISC 2025 results on this date: Here's how to check\n",
            "\n",
            "Processing: https://timesofindia.indiatimes.com/sports/cricket/news/former-pakistan-cricketer-slams-shahid-afridi-over-extremist-views-urges-indian-media-to-deny-him-platform/articleshow/120698363.cms\n",
            "Extracting content from: https://timesofindia.indiatimes.com/sports/cricket/news/former-pakistan-cricketer-slams-shahid-afridi-over-extremist-views-urges-indian-media-to-deny-him-platform/articleshow/120698363.cms\n",
            "Error extracting content from https://timesofindia.indiatimes.com/sports/cricket/news/former-pakistan-cricketer-slams-shahid-afridi-over-extremist-views-urges-indian-media-to-deny-him-platform/articleshow/120698363.cms: name 'Article' is not defined\n",
            "Primary extraction failed, attempting fallback...\n",
            "Attempting fallback extraction for: https://timesofindia.indiatimes.com/sports/cricket/news/former-pakistan-cricketer-slams-shahid-afridi-over-extremist-views-urges-indian-media-to-deny-him-platform/articleshow/120698363.cms\n",
            "Successfully extracted: Former Pakistan cricketer slams Shahid Afridi over 'extremist' views, urges Indian media to deny him platform\n",
            "\n",
            "Processing: https://timesofindia.indiatimes.com/sports/nfl/news/nfl-offseason-just-got-spicyand-george-kittles-76m-extension-is-at-the-center-of-it/articleshow/120735666.cms\n",
            "Extracting content from: https://timesofindia.indiatimes.com/sports/nfl/news/nfl-offseason-just-got-spicyand-george-kittles-76m-extension-is-at-the-center-of-it/articleshow/120735666.cms\n",
            "Error extracting content from https://timesofindia.indiatimes.com/sports/nfl/news/nfl-offseason-just-got-spicyand-george-kittles-76m-extension-is-at-the-center-of-it/articleshow/120735666.cms: name 'Article' is not defined\n",
            "Primary extraction failed, attempting fallback...\n",
            "Attempting fallback extraction for: https://timesofindia.indiatimes.com/sports/nfl/news/nfl-offseason-just-got-spicyand-george-kittles-76m-extension-is-at-the-center-of-it/articleshow/120735666.cms\n",
            "Successfully extracted: NFL offseason just got spicyâAnd George Kittleâs $76M extension is at the center of it\n",
            "\n",
            "Processing: https://timesofindia.indiatimes.com/sports/mlb/news/will-shohei-ohtani-play-tonight-against-miami-marlins-latest-update-on-los-angeles-dodgers-stars-injury-report-april-29-2025/articleshow/120720341.cms\n",
            "Extracting content from: https://timesofindia.indiatimes.com/sports/mlb/news/will-shohei-ohtani-play-tonight-against-miami-marlins-latest-update-on-los-angeles-dodgers-stars-injury-report-april-29-2025/articleshow/120720341.cms\n",
            "Error extracting content from https://timesofindia.indiatimes.com/sports/mlb/news/will-shohei-ohtani-play-tonight-against-miami-marlins-latest-update-on-los-angeles-dodgers-stars-injury-report-april-29-2025/articleshow/120720341.cms: name 'Article' is not defined\n",
            "Primary extraction failed, attempting fallback...\n",
            "Attempting fallback extraction for: https://timesofindia.indiatimes.com/sports/mlb/news/will-shohei-ohtani-play-tonight-against-miami-marlins-latest-update-on-los-angeles-dodgers-stars-injury-report-april-29-2025/articleshow/120720341.cms\n",
            "Successfully extracted: Will Shohei Ohtani play tonight against Miami Marlins? Latest update on Los Angeles Dodgers starâs injury report (April 29, 2025)\n",
            "\n",
            "========================================\n",
            "Crawling South China Morning Post...\n",
            "========================================\n",
            "Fetching articles from South China Morning Post (https://www.scmp.com/news/world)...\n",
            "Found 59 potential article URLs from South China Morning Post\n",
            "\n",
            "Processing: https://www.scmp.com/news/world/united-states-canada?module=sub_section_menu&pgtype=section\n",
            "Extracting content from: https://www.scmp.com/news/world/united-states-canada?module=sub_section_menu&pgtype=section\n",
            "Error extracting content from https://www.scmp.com/news/world/united-states-canada?module=sub_section_menu&pgtype=section: name 'Article' is not defined\n",
            "Primary extraction failed, attempting fallback...\n",
            "Attempting fallback extraction for: https://www.scmp.com/news/world/united-states-canada?module=sub_section_menu&pgtype=section\n",
            "Failed to extract content from https://www.scmp.com/news/world/united-states-canada?module=sub_section_menu&pgtype=section\n",
            "\n",
            "Processing: https://www.scmp.com/news/world/europe?module=sub_section_menu&pgtype=section\n",
            "Extracting content from: https://www.scmp.com/news/world/europe?module=sub_section_menu&pgtype=section\n",
            "Error extracting content from https://www.scmp.com/news/world/europe?module=sub_section_menu&pgtype=section: name 'Article' is not defined\n",
            "Primary extraction failed, attempting fallback...\n",
            "Attempting fallback extraction for: https://www.scmp.com/news/world/europe?module=sub_section_menu&pgtype=section\n",
            "Failed to extract content from https://www.scmp.com/news/world/europe?module=sub_section_menu&pgtype=section\n",
            "\n",
            "Processing: https://www.scmp.com/news/world/middle-east?module=sub_section_menu&pgtype=section\n",
            "Extracting content from: https://www.scmp.com/news/world/middle-east?module=sub_section_menu&pgtype=section\n",
            "Error extracting content from https://www.scmp.com/news/world/middle-east?module=sub_section_menu&pgtype=section: name 'Article' is not defined\n",
            "Primary extraction failed, attempting fallback...\n",
            "Attempting fallback extraction for: https://www.scmp.com/news/world/middle-east?module=sub_section_menu&pgtype=section\n",
            "Failed to extract content from https://www.scmp.com/news/world/middle-east?module=sub_section_menu&pgtype=section\n",
            "\n",
            "Processing: https://www.scmp.com/news/world/americas?module=sub_section_menu&pgtype=section\n",
            "Extracting content from: https://www.scmp.com/news/world/americas?module=sub_section_menu&pgtype=section\n",
            "Error extracting content from https://www.scmp.com/news/world/americas?module=sub_section_menu&pgtype=section: name 'Article' is not defined\n",
            "Primary extraction failed, attempting fallback...\n",
            "Attempting fallback extraction for: https://www.scmp.com/news/world/americas?module=sub_section_menu&pgtype=section\n",
            "Failed to extract content from https://www.scmp.com/news/world/americas?module=sub_section_menu&pgtype=section\n",
            "\n",
            "Processing: https://www.scmp.com/news/world/africa?module=sub_section_menu&pgtype=section\n",
            "Extracting content from: https://www.scmp.com/news/world/africa?module=sub_section_menu&pgtype=section\n",
            "Error extracting content from https://www.scmp.com/news/world/africa?module=sub_section_menu&pgtype=section: name 'Article' is not defined\n",
            "Primary extraction failed, attempting fallback...\n",
            "Attempting fallback extraction for: https://www.scmp.com/news/world/africa?module=sub_section_menu&pgtype=section\n",
            "Failed to extract content from https://www.scmp.com/news/world/africa?module=sub_section_menu&pgtype=section\n",
            "\n",
            "--- Crawling Complete ---\n",
            "Total articles collected: 36\n",
            "Articles by source:\n",
            "source\n",
            "Los Angeles Times     5\n",
            "The Guardian          5\n",
            "BBC News              5\n",
            "The Independent       5\n",
            "CNN                   5\n",
            "Al Jazeera            5\n",
            "The Times of India    4\n",
            "USA Today             2\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Dataset saved to 'newspaper_dataset.csv'\n",
            "Dataset saved to SQLite database 'newspaper_articles.db'\n",
            "\n",
            "--- Sample Data ---\n",
            "                                               title             source  \\\n",
            "0  Watch terrifying moment as man narrowly escape...          USA Today   \n",
            "1  What to know about the Sistine Chapel, site of...          USA Today   \n",
            "2                                      Art & Culture  Los Angeles Times   \n",
            "3                                      Conversations  Los Angeles Times   \n",
            "4                              Your L.A. event guide  Los Angeles Times   \n",
            "\n",
            "                publish_date  authors  potential_fake_news_score  \n",
            "0                             Unknown                   7.062147  \n",
            "1                             Unknown                   0.000000  \n",
            "2  Sept. 13, 2022 1:40 PM PT  Unknown                   0.000000  \n",
            "3             April 22, 2025  Unknown                   0.000000  \n",
            "4  Sept. 13, 2022 1:23 PM PT  Unknown                   0.000000  \n",
            "\n",
            "--- Fake News Analysis Results ---\n",
            "Average potential fake news score: 4.91/100\n",
            "\n",
            "Articles with highest fake news indicators:\n",
            "- Will Shohei Ohtani play tonight against Miami Marlins? Latest update on Los Angeles Dodgers starâs injury report (April 29, 2025) (The Times of India) - Score: 28.01\n",
            "- Former Pakistan cricketer slams Shahid Afridi over 'extremist' views, urges Indian media to deny him platform (The Times of India) - Score: 27.78\n",
            "- NFL offseason just got spicyâAnd George Kittleâs $76M extension is at the center of it (The Times of India) - Score: 27.43\n",
            "- At least three killed in Sweden shooting: Police (Al Jazeera) - Score: 16.10\n",
            "- Israel releases Gaza paramedic who survived deadly attack on health workers (Al Jazeera) - Score: 12.67\n",
            "\n",
            "Articles with lowest fake news indicators:\n",
            "- What to know about the Sistine Chapel, site of the conclave to pick a new pope (USA Today) - Score: 0.00\n",
            "- Art & Culture (Los Angeles Times) - Score: 0.00\n",
            "- Conversations (Los Angeles Times) - Score: 0.00\n",
            "- Your L.A. event guide (Los Angeles Times) - Score: 0.00\n",
            "- Styling Myself (Los Angeles Times) - Score: 0.00\n",
            "\n",
            "Crawling process completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-129660fdd6a2>:311: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '21.186440677966104' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  df.at[idx, 'emotional_score'] = min((emotional_count / max(len(text.split()) / 100, 1)) * 100, 100)\n",
            "<ipython-input-6-129660fdd6a2>:315: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '29.197080291970806' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  df.at[idx, 'sensational_score'] = min((sensational_count / max(len(text.split()) / 200, 1)) * 100, 100)\n"
          ]
        }
      ]
    }
  ]
}