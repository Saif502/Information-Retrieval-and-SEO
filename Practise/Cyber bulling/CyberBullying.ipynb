{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9CJEbEcT1rG",
        "outputId": "a35cfcf8-8cd2-45e3-ebc9-f33a518221ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Collecting bnlp-toolkit\n",
            "  Downloading bnlp_toolkit-4.0.3-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: sentencepiece==0.2.0 in /usr/local/lib/python3.11/dist-packages (from bnlp-toolkit) (0.2.0)\n",
            "Collecting gensim==4.3.2 (from bnlp-toolkit)\n",
            "  Downloading gensim-4.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from bnlp-toolkit) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bnlp-toolkit) (2.0.2)\n",
            "Collecting scipy==1.10.1 (from bnlp-toolkit)\n",
            "  Downloading scipy-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sklearn-crfsuite==0.3.6 (from bnlp-toolkit)\n",
            "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting tqdm==4.66.3 (from bnlp-toolkit)\n",
            "  Downloading tqdm-4.66.3-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ftfy==6.2.0 (from bnlp-toolkit)\n",
            "  Downloading ftfy-6.2.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting emoji==1.7.0 (from bnlp-toolkit)\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bnlp-toolkit) (2.32.3)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.11/dist-packages (from ftfy==6.2.0->bnlp-toolkit) (0.2.13)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim==4.3.2->bnlp-toolkit) (7.1.0)\n",
            "Collecting numpy (from bnlp-toolkit)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-crfsuite>=0.8.3 (from sklearn-crfsuite==0.3.6->bnlp-toolkit)\n",
            "  Downloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from sklearn-crfsuite==0.3.6->bnlp-toolkit) (1.17.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from sklearn-crfsuite==0.3.6->bnlp-toolkit) (0.9.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->bnlp-toolkit) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->bnlp-toolkit) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->bnlp-toolkit) (2024.11.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bnlp-toolkit) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bnlp-toolkit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bnlp-toolkit) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bnlp-toolkit) (2025.4.26)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim==4.3.2->bnlp-toolkit) (1.17.2)\n",
            "Downloading bnlp_toolkit-4.0.3-py3-none-any.whl (22 kB)\n",
            "Downloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gensim-4.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.1/34.1 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
            "Downloading tqdm-4.66.3-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171031 sha256=736474c20326a6b4ea70df23b2112378034c94d0da6376bdfb3e2945cbca17cd\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/22/e5/b69726d5e1a19795ecd3b3e7464b16c0f1d019aa94ff1c8578\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji, tqdm, python-crfsuite, numpy, ftfy, sklearn-crfsuite, scipy, gensim, bnlp-toolkit\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.67.1\n",
            "    Uninstalling tqdm-4.67.1:\n",
            "      Successfully uninstalled tqdm-4.67.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.2\n",
            "    Uninstalling scipy-1.15.2:\n",
            "      Successfully uninstalled scipy-1.15.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "scikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.10.1 which is incompatible.\n",
            "dataproc-spark-connect 0.7.2 requires tqdm>=4.67, but you have tqdm 4.66.3 which is incompatible.\n",
            "jax 0.5.2 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\n",
            "jaxlib 0.5.1 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\n",
            "cvxpy 1.6.5 requires scipy>=1.11.0, but you have scipy 1.10.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bnlp-toolkit-4.0.3 emoji-1.7.0 ftfy-6.2.0 gensim-4.3.2 numpy-1.26.4 python-crfsuite-0.9.11 scipy-1.10.1 sklearn-crfsuite-0.3.6 tqdm-4.66.3\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.11/dist-packages (1.7.0)\n"
          ]
        }
      ],
      "source": [
        " !pip install pandas\n",
        " !pip install openpyxl  # For Excel files\n",
        " !pip install beautifulsoup4\n",
        " !pip install nltk\n",
        " !pip install scikit-learn\n",
        " !pip install bnlp-toolkit  # Bengali NLP tools (if available)\n",
        " !pip install emoji  # For emoji handling\n",
        "\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "import emoji\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Before running the script, install these packages:\n",
        "\n",
        "# Try to import Bengali NLP tools (if available)\n",
        "try:\n",
        "    from bnlp.corpus import stopwords\n",
        "    from bnlp.stemmer import BanglaStemmer\n",
        "    BENGALI_NLP_AVAILABLE = True\n",
        "    bengali_stemmer = BanglaStemmer()\n",
        "    bengali_stopwords = set(stopwords())\n",
        "    print(\"Bengali NLP tools loaded successfully.\")\n",
        "except ImportError:\n",
        "    BENGALI_NLP_AVAILABLE = False\n",
        "    print(\"Bengali NLP tools not available. Using basic preprocessing only.\")\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load Bangla Cyberbullying dataset (XLSX format)\n",
        "print(\"Loading XLSX dataset...\")\n",
        "excel_file = \"/content/Bangla Cyberbullying Dataset.xlsx\"  # Make sure this matches your file name\n",
        "if not os.path.exists(excel_file):\n",
        "    # Try to find any Excel file in the current directory\n",
        "    excel_files = [f for f in os.listdir('.') if f.endswith('.xlsx')]\n",
        "    if excel_files:\n",
        "        excel_file = excel_files[0]\n",
        "        print(f\"Using found Excel file: {excel_file}\")\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"No Excel file found. Please ensure your dataset is in the current directory.\")\n",
        "\n",
        "df = pd.read_excel(excel_file)\n",
        "\n",
        "# Initialize log_steps for storing steps\n",
        "log_steps = []\n",
        "\n",
        "# Check the column names in your dataset\n",
        "print(\"Columns in the dataset:\", df.columns.tolist())\n",
        "\n",
        "# For this cyberbullying dataset, determine which column contains the text\n",
        "# Common names might be 'text', 'comment', 'content', etc.\n",
        "content_columns = [col for col in df.columns if col.lower() in ['text', 'comment', 'content', 'message', 'post']]\n",
        "if content_columns:\n",
        "    content_column = content_columns[0]\n",
        "    print(f\"Using '{content_column}' as the text content column\")\n",
        "else:\n",
        "    # If no standard column name is found, use the first non-label column as a guess\n",
        "    non_label_cols = [col for col in df.columns if col.lower() not in ['label', 'class', 'target', 'category']]\n",
        "    if non_label_cols:\n",
        "        content_column = non_label_cols[0]\n",
        "        print(f\"No standard text column found. Using '{content_column}' as the text content column\")\n",
        "    else:\n",
        "        content_column = df.columns[0]\n",
        "        print(f\"Using first column '{content_column}' as the text content column\")\n",
        "\n",
        "# Sample validation - check if we have actual text content\n",
        "sample_text = df[content_column].astype(str).iloc[0]\n",
        "if len(sample_text.strip()) < 5:  # Very short text may indicate wrong column\n",
        "    print(f\"WARNING: Selected content column '{content_column}' may not contain text (first sample is very short)\")\n",
        "\n",
        "# Example text for logging\n",
        "example_text = sample_text\n",
        "\n",
        "def log_step(title, content):\n",
        "    log_steps.append(f\"\\n--- {title} ---\\n{content[:1000]}...\\n\")  # Limit log length\n",
        "\n",
        "log_step(\"Original Text\", example_text)\n",
        "\n",
        "# Step 1: HTML Parsing\n",
        "html_parsed = BeautifulSoup(example_text, \"html.parser\").get_text()\n",
        "log_step(\"HTML Parsed\", html_parsed)\n",
        "\n",
        "# Step 2: Remove URLs\n",
        "url_pattern = r'https?://\\S+|www\\.\\S+'\n",
        "no_urls = re.sub(url_pattern, '', html_parsed)\n",
        "log_step(\"URLs Removed\", no_urls)\n",
        "\n",
        "# Step 3: Remove emojis\n",
        "def remove_emoji(text):\n",
        "    return emoji.replace_emoji(text, replace='')\n",
        "\n",
        "no_emoji = remove_emoji(no_urls)\n",
        "log_step(\"Emojis Removed\", no_emoji)\n",
        "\n",
        "# Step 4: Remove special characters (keeping Bangla Unicode range)\n",
        "# Bangla Unicode range: \\u0980-\\u09FF\n",
        "def remove_special_chars(text):\n",
        "    # Keep Bangla characters, digits, and spaces\n",
        "    return re.sub(r'[^\\u0980-\\u09FF\\s\\d]', '', text)\n",
        "\n",
        "no_special = remove_special_chars(no_emoji)\n",
        "log_step(\"Special Characters Removed\", no_special)\n",
        "\n",
        "# Step 5: Tokenization (simple space-based for Bangla)\n",
        "tokens = no_special.split()\n",
        "log_step(\"Tokenized\", str(tokens[:100]) + \"...\")  # Only show first 100 tokens\n",
        "\n",
        "# Step 6: Stopword Removal (if Bengali NLP tools are available)\n",
        "if BENGALI_NLP_AVAILABLE:\n",
        "    filtered = [word for word in tokens if word not in bengali_stopwords]\n",
        "    log_step(\"Stopwords Removed\", str(filtered[:100]) + \"...\")  # Only show first 100 tokens\n",
        "else:\n",
        "    filtered = tokens  # Skip stopword removal if tools aren't available\n",
        "    log_step(\"Stopwords Removal (Skipped)\", \"Bengali stopwords list not available\")\n",
        "\n",
        "# Step 7: Stemming (if Bengali NLP tools are available)\n",
        "if BENGALI_NLP_AVAILABLE:\n",
        "    stemmed = [bengali_stemmer.stem(word) for word in filtered]\n",
        "    final_text = ' '.join(stemmed)\n",
        "    log_step(\"Stemmed\", final_text[:1000] + \"...\")  # Only show first 1000 chars\n",
        "else:\n",
        "    final_text = ' '.join(filtered)  # Skip stemming if tools aren't available\n",
        "    log_step(\"Stemming (Skipped)\", \"Bengali stemmer not available\")\n",
        "\n",
        "# Clean function for full dataset\n",
        "def clean_bangla_text(text):\n",
        "    text = str(text)\n",
        "    text = BeautifulSoup(text, \"html.parser\").get_text()  # HTML removal\n",
        "    text = re.sub(url_pattern, '', text)  # URL removal\n",
        "    text = remove_emoji(text)  # Emoji removal\n",
        "    text = remove_special_chars(text)  # Special character removal\n",
        "\n",
        "    tokens = text.split()  # Tokenization\n",
        "\n",
        "    # Apply stopword removal if available\n",
        "    if BENGALI_NLP_AVAILABLE:\n",
        "        tokens = [word for word in tokens if word not in bengali_stopwords]\n",
        "\n",
        "    # Apply stemming if available\n",
        "    if BENGALI_NLP_AVAILABLE:\n",
        "        tokens = [bengali_stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply cleaning\n",
        "print(\"Cleaning text data...\")\n",
        "df[\"cleaned_text\"] = df[content_column].astype(str).apply(clean_bangla_text)\n",
        "\n",
        "# Save cleaned dataset (as CSV for broader compatibility)\n",
        "print(\"Saving cleaned dataset...\")\n",
        "df.to_csv(\"bangla_cyberbullying_cleaned.csv\", index=False)\n",
        "\n",
        "# Also save as Excel if preferred\n",
        "df.to_excel(\"bangla_cyberbullying_cleaned.xlsx\", index=False)\n",
        "\n",
        "# Limit dataset size to avoid memory issues\n",
        "max_samples = min(10000, len(df))  # Adjust based on your system's memory\n",
        "print(f\"Using {max_samples} samples for TF-IDF analysis...\")\n",
        "df_sample = df[\"cleaned_text\"].dropna().iloc[:max_samples]\n",
        "\n",
        "# TF-IDF vectorization (optimized)\n",
        "print(\"Performing TF-IDF vectorization...\")\n",
        "vectorizer = TfidfVectorizer(max_features=1000)  # Limit features to reduce memory\n",
        "tfidf_matrix = vectorizer.fit_transform(df_sample)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Calculate average TF-IDF weights\n",
        "avg_weights = tfidf_matrix.mean(axis=0).A1  # Convert to 1D array\n",
        "feature_scores = pd.Series(avg_weights, index=feature_names).sort_values(ascending=False)\n",
        "\n",
        "# Get top 30 words and bottom 30 words\n",
        "top_features = feature_scores.head(30)\n",
        "bottom_features = feature_scores.tail(30)\n",
        "\n",
        "# Write processing log\n",
        "print(\"Writing processing log...\")\n",
        "with open(\"processing_log.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"==== TEXT CLEANING LOG ====\\n\")\n",
        "    f.write(\"Steps Applied to Each Text Entry:\\n\")\n",
        "    f.write(\"1. HTML Parsing\\n\")\n",
        "    f.write(\"2. URL Removal\\n\")\n",
        "    f.write(\"3. Emoji Removal\\n\")\n",
        "    f.write(\"4. Special Character Removal (preserving Bangla Unicode)\\n\")\n",
        "    f.write(\"5. Tokenization\\n\")\n",
        "    if BENGALI_NLP_AVAILABLE:\n",
        "        f.write(\"6. Stopword Removal\\n\")\n",
        "        f.write(\"7. Stemming\\n\")\n",
        "    else:\n",
        "        f.write(\"6. Stopword Removal (SKIPPED - tools not available)\\n\")\n",
        "        f.write(\"7. Stemming (SKIPPED - tools not available)\\n\")\n",
        "    f.write(\"\\n==== Example Text Processing ====\\n\")\n",
        "    f.write('\\n'.join(log_steps))\n",
        "    f.write(\"\\n\\n==== TF-IDF SUMMARY ====\\n\")\n",
        "    f.write(f\"TF-IDF Matrix Shape: {tfidf_matrix.shape}\\n\")\n",
        "    f.write(\"Top 30 TF-IDF features:\\n\")\n",
        "    f.write(top_features.to_string())\n",
        "\n",
        "# Write TF-IDF summary\n",
        "print(\"Writing TF-IDF summary...\")\n",
        "with open(\"tfidf_summary.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(f\"TF-IDF Matrix Shape: {tfidf_matrix.shape}\\n\\n\")\n",
        "    f.write(\"Top 30 TF-IDF features by average weight:\\n\")\n",
        "    f.write(top_features.to_string())\n",
        "    f.write(\"\\n\\n\")\n",
        "    f.write(\"Bottom 30 TF-IDF features by average weight:\\n\")\n",
        "    f.write(bottom_features.to_string())\n",
        "\n",
        "# Additional analysis: Separate bullying and non-bullying (if labels exist)\n",
        "# Look for typical label column names\n",
        "label_columns = [col for col in df.columns if col.lower() in ['label', 'class', 'target', 'category', 'cyberbullying', 'bullying', 'bully']]\n",
        "\n",
        "if label_columns:\n",
        "    label_column = label_columns[0]\n",
        "    print(f\"Performing analysis by {label_column} category...\")\n",
        "\n",
        "    # Get unique categories\n",
        "    categories = df[label_column].unique()\n",
        "\n",
        "    with open(\"category_analysis.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"==== CATEGORY-BASED ANALYSIS ====\\n\\n\")\n",
        "\n",
        "        for category in categories:\n",
        "            f.write(f\"Category: {category}\\n\")\n",
        "\n",
        "            # Get samples for this category\n",
        "            category_samples = df[df[label_column] == category][\"cleaned_text\"].dropna()\n",
        "            if len(category_samples) > 0:\n",
        "                # Limit samples to avoid memory issues\n",
        "                category_samples = category_samples.iloc[:min(5000, len(category_samples))]\n",
        "\n",
        "                # TF-IDF for this category\n",
        "                cat_vectorizer = TfidfVectorizer(max_features=500)\n",
        "                cat_tfidf = cat_vectorizer.fit_transform(category_samples)\n",
        "                cat_features = cat_vectorizer.get_feature_names_out()\n",
        "\n",
        "                # Calculate average weights\n",
        "                cat_weights = cat_tfidf.mean(axis=0).A1\n",
        "                cat_scores = pd.Series(cat_weights, index=cat_features).sort_values(ascending=False)\n",
        "\n",
        "                # Write top 20 features for this category\n",
        "                f.write(f\"Top 20 TF-IDF features for category '{category}':\\n\")\n",
        "                f.write(cat_scores.head(20).to_string())\n",
        "                f.write(\"\\n\\n\")\n",
        "            else:\n",
        "                f.write(f\"No samples found for category '{category}'\\n\\n\")\n",
        "\n",
        "    print(\"- category_analysis.txt\")\n",
        "else:\n",
        "    print(\"No label column identified for category analysis\")\n",
        "\n",
        "print(\"✅ All files generated:\")\n",
        "print(\"- bangla_cyberbullying_cleaned.csv\")\n",
        "print(\"- bangla_cyberbullying_cleaned.xlsx\")\n",
        "print(\"- processing_log.txt\")\n",
        "print(\"- tfidf_summary.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjCyq6tqT3D1",
        "outputId": "ae4a7bcb-09f4-485a-db9e-d070cb7bceed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "punkt not found. downloading...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bengali NLP tools not available. Using basic preprocessing only.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading XLSX dataset...\n",
            "Columns in the dataset: ['No.', 'Text', 'Label']\n",
            "Using 'Text' as the text content column\n",
            "Cleaning text data...\n",
            "Saving cleaned dataset...\n",
            "Using 10000 samples for TF-IDF analysis...\n",
            "Performing TF-IDF vectorization...\n",
            "Writing processing log...\n",
            "Writing TF-IDF summary...\n",
            "Performing analysis by Label category...\n",
            "- category_analysis.txt\n",
            "✅ All files generated:\n",
            "- bangla_cyberbullying_cleaned.csv\n",
            "- bangla_cyberbullying_cleaned.xlsx\n",
            "- processing_log.txt\n",
            "- tfidf_summary.txt\n"
          ]
        }
      ]
    }
  ]
}